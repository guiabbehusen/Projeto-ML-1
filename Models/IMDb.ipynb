{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ffe268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2419462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7f06b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7363fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfd1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54b4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaf175b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac873be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fcd13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5cf97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset.\n",
       "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_path='C:\\\\Users\\\\User\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94bf3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc61cfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b22a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5de993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a59af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8a96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279412af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b099f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e44bee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'the',\n",
       " b'a',\n",
       " b'of',\n",
       " b'and',\n",
       " b'to',\n",
       " b'I',\n",
       " b'is',\n",
       " b'in',\n",
       " b'this',\n",
       " b'it',\n",
       " b'was',\n",
       " b'movie',\n",
       " b'that',\n",
       " b'The',\n",
       " b'film',\n",
       " b'with',\n",
       " b'for',\n",
       " b'as',\n",
       " b'on',\n",
       " b'but',\n",
       " b'have',\n",
       " b'This',\n",
       " b'one',\n",
       " b'not',\n",
       " b'be',\n",
       " b'are',\n",
       " b'you',\n",
       " b'an',\n",
       " b'at',\n",
       " b'about',\n",
       " b'by',\n",
       " b'all',\n",
       " b'his',\n",
       " b'so',\n",
       " b'like',\n",
       " b'from',\n",
       " b'who',\n",
       " b'has',\n",
       " b'It',\n",
       " b'good',\n",
       " b'my',\n",
       " b'just',\n",
       " b'very',\n",
       " b'out',\n",
       " b'or',\n",
       " b'story',\n",
       " b'some',\n",
       " b'time',\n",
       " b'had',\n",
       " b'he',\n",
       " b'they',\n",
       " b'really',\n",
       " b'me',\n",
       " b'when',\n",
       " b'what',\n",
       " b'first',\n",
       " b'movies',\n",
       " b'bad',\n",
       " b'see',\n",
       " b'seen',\n",
       " b'up',\n",
       " b'only',\n",
       " b'were',\n",
       " b\"it's\",\n",
       " b'would',\n",
       " b'more',\n",
       " b'made',\n",
       " b'great',\n",
       " b'can',\n",
       " b'been',\n",
       " b'i',\n",
       " b'her',\n",
       " b'no',\n",
       " b'A',\n",
       " b'which',\n",
       " b'even',\n",
       " b'films',\n",
       " b'there',\n",
       " b'ever',\n",
       " b'people',\n",
       " b'much',\n",
       " b'because',\n",
       " b'most',\n",
       " b'plot',\n",
       " b'if',\n",
       " b'than',\n",
       " b'acting',\n",
       " b'get',\n",
       " b'their',\n",
       " b'well',\n",
       " b'into',\n",
       " b'how',\n",
       " b'best',\n",
       " b'think',\n",
       " b'other',\n",
       " b'its',\n",
       " b\"It's\",\n",
       " b'saw',\n",
       " b'could',\n",
       " b'watch',\n",
       " b'many',\n",
       " b\"don't\",\n",
       " b'do',\n",
       " b'will',\n",
       " b'say',\n",
       " b'show',\n",
       " b'love',\n",
       " b'make',\n",
       " b'two',\n",
       " b'way',\n",
       " b'know',\n",
       " b'years',\n",
       " b'after',\n",
       " b'But',\n",
       " b'we',\n",
       " b'watching',\n",
       " b'In',\n",
       " b'never',\n",
       " b'any',\n",
       " b'life',\n",
       " b'characters',\n",
       " b\"I'm\",\n",
       " b'too',\n",
       " b'did',\n",
       " b\"I've\",\n",
       " b'little',\n",
       " b'over',\n",
       " b'If',\n",
       " b'she',\n",
       " b'then',\n",
       " b'being',\n",
       " b'thought',\n",
       " b'them',\n",
       " b'off',\n",
       " b'There',\n",
       " b's',\n",
       " b'funny',\n",
       " b'better',\n",
       " b'also',\n",
       " b'watched',\n",
       " b'where',\n",
       " b'still',\n",
       " b'worst',\n",
       " b'old',\n",
       " b'am',\n",
       " b'through',\n",
       " b'actors',\n",
       " b'And',\n",
       " b'character',\n",
       " b'such',\n",
       " b\"didn't\",\n",
       " b'back',\n",
       " b'got',\n",
       " b'horror',\n",
       " b'those',\n",
       " b'series',\n",
       " b'lot',\n",
       " b'should',\n",
       " b'comedy',\n",
       " b'go',\n",
       " b'your',\n",
       " b'few',\n",
       " b'something',\n",
       " b'does',\n",
       " b'thing',\n",
       " b'him',\n",
       " b'man',\n",
       " b'What',\n",
       " b'going',\n",
       " b'original',\n",
       " b'director',\n",
       " b'work',\n",
       " b'here',\n",
       " b'before',\n",
       " b'real',\n",
       " b'actually',\n",
       " b'cast',\n",
       " b'pretty',\n",
       " b'TV',\n",
       " b'quite',\n",
       " b'why',\n",
       " b'every',\n",
       " b'He',\n",
       " b'find',\n",
       " b'As',\n",
       " b'scenes',\n",
       " b'young',\n",
       " b\"can't\",\n",
       " b'interesting',\n",
       " b'same',\n",
       " b'fan',\n",
       " b'now',\n",
       " b'makes',\n",
       " b'end',\n",
       " b'big',\n",
       " b'When',\n",
       " b'long',\n",
       " b'nothing',\n",
       " b'times',\n",
       " b'found',\n",
       " b\"'s\",\n",
       " b'must',\n",
       " b'these',\n",
       " b'again',\n",
       " b'another',\n",
       " b'part',\n",
       " b'action',\n",
       " b'though',\n",
       " b'always',\n",
       " b'script',\n",
       " b\"doesn't\",\n",
       " b'last',\n",
       " b'new',\n",
       " b'fact',\n",
       " b'year',\n",
       " b'while',\n",
       " b'around',\n",
       " b'look',\n",
       " b'since',\n",
       " b't',\n",
       " b'minutes',\n",
       " b'family',\n",
       " b'DVD',\n",
       " b'down',\n",
       " b'scene',\n",
       " b'things',\n",
       " b'want',\n",
       " b'read',\n",
       " b'seems',\n",
       " b'My',\n",
       " b'fun',\n",
       " b'may',\n",
       " b'world',\n",
       " b'done',\n",
       " b'far',\n",
       " b'come',\n",
       " b'right',\n",
       " b'take',\n",
       " b\"wasn't\",\n",
       " b'You',\n",
       " b'th',\n",
       " b'believe',\n",
       " b'almost',\n",
       " b'enough',\n",
       " b'hard',\n",
       " b'whole',\n",
       " b'anything',\n",
       " b'book',\n",
       " b\"isn't\",\n",
       " b'probably',\n",
       " b'least',\n",
       " b'give',\n",
       " b'bit',\n",
       " b'They',\n",
       " b'American',\n",
       " b'seeing',\n",
       " b'without',\n",
       " b'feel',\n",
       " b'One',\n",
       " b'awful',\n",
       " b'idea',\n",
       " b'making',\n",
       " b'between',\n",
       " b'remember',\n",
       " b'came',\n",
       " b'music',\n",
       " b'kind',\n",
       " b'set',\n",
       " b'excellent',\n",
       " b'John',\n",
       " b'classic',\n",
       " b'us',\n",
       " b'both',\n",
       " b'Well',\n",
       " b'version',\n",
       " b'performance',\n",
       " b'gets',\n",
       " b'plays',\n",
       " b'day',\n",
       " b'Not',\n",
       " b'low',\n",
       " b'might',\n",
       " b'own',\n",
       " b'loved',\n",
       " b'budget',\n",
       " b'role',\n",
       " b'reason',\n",
       " b'high',\n",
       " b'First',\n",
       " b'girl',\n",
       " b'shows',\n",
       " b'looking',\n",
       " b'After',\n",
       " b'For',\n",
       " b'So',\n",
       " b'video',\n",
       " b'special',\n",
       " b'However',\n",
       " b'effects',\n",
       " b'woman',\n",
       " b'different',\n",
       " b'terrible',\n",
       " b'true',\n",
       " b'point',\n",
       " b'start',\n",
       " b'actor',\n",
       " b'put',\n",
       " b'night',\n",
       " b'three',\n",
       " b'played',\n",
       " b'liked',\n",
       " b'yet',\n",
       " b'boring',\n",
       " b'beautiful',\n",
       " b'takes',\n",
       " b'especially',\n",
       " b'based',\n",
       " b'ago',\n",
       " b'main',\n",
       " b'short',\n",
       " b'play',\n",
       " b'guy',\n",
       " b'title',\n",
       " b'said',\n",
       " b'enjoyed',\n",
       " b'anyone',\n",
       " b'wonderful',\n",
       " b'trying',\n",
       " b\"that's\",\n",
       " b'sure',\n",
       " b'went',\n",
       " b'favorite',\n",
       " b'having',\n",
       " b'stars',\n",
       " b'together',\n",
       " b'Hollywood',\n",
       " b'absolutely',\n",
       " b'screen',\n",
       " b'our',\n",
       " b'wrong',\n",
       " b'rather',\n",
       " b'half',\n",
       " b'production',\n",
       " b'episode',\n",
       " b'goes',\n",
       " b'place',\n",
       " b'comes',\n",
       " b'early',\n",
       " b'piece',\n",
       " b'written',\n",
       " b'We',\n",
       " b'heard',\n",
       " b'away',\n",
       " b\"couldn't\",\n",
       " b'second',\n",
       " b'money',\n",
       " b'during',\n",
       " b'friends',\n",
       " b'job',\n",
       " b'THE',\n",
       " b'someone',\n",
       " b'entertaining',\n",
       " b'sense',\n",
       " b'looks',\n",
       " b'She',\n",
       " b'line',\n",
       " b'school',\n",
       " b'drama',\n",
       " b'each',\n",
       " b'flick',\n",
       " b'everything',\n",
       " b'understand',\n",
       " b'No',\n",
       " b'mean',\n",
       " b'course',\n",
       " b'enjoy',\n",
       " b'star',\n",
       " b'wanted',\n",
       " b'wife',\n",
       " b'directed',\n",
       " b'That',\n",
       " b'To',\n",
       " b'small',\n",
       " b'mind',\n",
       " b'seem',\n",
       " b'truly',\n",
       " b'While',\n",
       " b'stupid',\n",
       " b'simply',\n",
       " b\"'\",\n",
       " b'poor',\n",
       " b'All',\n",
       " b'worth',\n",
       " b'everyone',\n",
       " b'once',\n",
       " b'worse',\n",
       " b'top',\n",
       " b'completely',\n",
       " b'used',\n",
       " b'cinema',\n",
       " b'felt',\n",
       " b'kids',\n",
       " b'genre',\n",
       " b'shot',\n",
       " b'home',\n",
       " b'name',\n",
       " b'late',\n",
       " b'tell',\n",
       " b'called',\n",
       " b'until',\n",
       " b'reviews',\n",
       " b'New',\n",
       " b'myself',\n",
       " b'beginning',\n",
       " b'full',\n",
       " b'nice',\n",
       " b'live',\n",
       " b'left',\n",
       " b'At',\n",
       " b'horrible',\n",
       " b'novel',\n",
       " b'How',\n",
       " b'Although',\n",
       " b'thriller',\n",
       " b'documentary',\n",
       " b'women',\n",
       " b'starts',\n",
       " b'performances',\n",
       " b'several',\n",
       " b'else',\n",
       " b'Even',\n",
       " b'audience',\n",
       " b'less',\n",
       " b'perfect',\n",
       " b'often',\n",
       " b'couple',\n",
       " b'writing',\n",
       " b'amazing',\n",
       " b\"I'd\",\n",
       " b'review',\n",
       " b'released',\n",
       " b'seemed',\n",
       " b'history',\n",
       " b'need',\n",
       " b'case',\n",
       " b'Some',\n",
       " b'started',\n",
       " b'later',\n",
       " b'war',\n",
       " b'waste',\n",
       " b'Michael',\n",
       " b'try',\n",
       " b'use',\n",
       " b'either',\n",
       " b'given',\n",
       " b'humor',\n",
       " b'comments',\n",
       " b'B',\n",
       " b'black',\n",
       " b'disappointed',\n",
       " b'although',\n",
       " b'laugh',\n",
       " b'father',\n",
       " b'along',\n",
       " b'gave',\n",
       " b'however',\n",
       " b'quality',\n",
       " b'b',\n",
       " b'expect',\n",
       " b'totally',\n",
       " b'friend',\n",
       " b'gives',\n",
       " b'OK',\n",
       " b'itself',\n",
       " b'sex',\n",
       " b'thinking',\n",
       " b'surprised',\n",
       " b\"you're\",\n",
       " b'brilliant',\n",
       " b'With',\n",
       " b'Now',\n",
       " b'others',\n",
       " b'definitely',\n",
       " b'entire',\n",
       " b'dialogue',\n",
       " b'days',\n",
       " b'ending',\n",
       " b\"he's\",\n",
       " b'television',\n",
       " b'already',\n",
       " b'getting',\n",
       " b'past',\n",
       " b'let',\n",
       " b'James',\n",
       " b'boy',\n",
       " b'camera',\n",
       " b'style',\n",
       " b'problem',\n",
       " b'certainly',\n",
       " b'Just',\n",
       " b'group',\n",
       " b'stories',\n",
       " b'lead',\n",
       " b'o',\n",
       " b'town',\n",
       " b'Why',\n",
       " b'Of',\n",
       " b'maybe',\n",
       " b'person',\n",
       " b'himself',\n",
       " b'playing',\n",
       " b'lost',\n",
       " b'art',\n",
       " b'supposed',\n",
       " b'become',\n",
       " b'doing',\n",
       " b'usually',\n",
       " b'lives',\n",
       " b'men',\n",
       " b'looked',\n",
       " b'moments',\n",
       " b'death',\n",
       " b'hit',\n",
       " b'kid',\n",
       " b'took',\n",
       " b'slow',\n",
       " b'house',\n",
       " b'known',\n",
       " b'sort',\n",
       " b'fine',\n",
       " b'example',\n",
       " b'recently',\n",
       " b'picture',\n",
       " b'premise',\n",
       " b'An',\n",
       " b'His',\n",
       " b'crap',\n",
       " b'knew',\n",
       " b'British',\n",
       " b'direction',\n",
       " b'Robert',\n",
       " b'child',\n",
       " b'decided',\n",
       " b'hour',\n",
       " b'parts',\n",
       " b'Mr',\n",
       " b'help',\n",
       " b'huge',\n",
       " b'expecting',\n",
       " b'From',\n",
       " b'guess',\n",
       " b'fans',\n",
       " b'lines',\n",
       " b'Man',\n",
       " b'works',\n",
       " b'son',\n",
       " b'human',\n",
       " b'attempt',\n",
       " b'David',\n",
       " b'children',\n",
       " b'decent',\n",
       " b'comment',\n",
       " b'extremely',\n",
       " b'finally',\n",
       " b'sequel',\n",
       " b'jokes',\n",
       " b'keep',\n",
       " b'admit',\n",
       " b'actress',\n",
       " b'etc',\n",
       " b'writer',\n",
       " b\"there's\",\n",
       " b'episodes',\n",
       " b'against',\n",
       " b'type',\n",
       " b'game',\n",
       " b'simple',\n",
       " b'musical',\n",
       " b'heart',\n",
       " b'George',\n",
       " b'turned',\n",
       " b'Yes',\n",
       " b'tale',\n",
       " b'storyline',\n",
       " b'under',\n",
       " b'greatest',\n",
       " b'age',\n",
       " b'romantic',\n",
       " b'run',\n",
       " b'opinion',\n",
       " b'rest',\n",
       " b'hilarious',\n",
       " b'Richard',\n",
       " b'able',\n",
       " b'perhaps',\n",
       " b'expected',\n",
       " b'w',\n",
       " b'saying',\n",
       " b'close',\n",
       " b'theater',\n",
       " b'act',\n",
       " b'opening',\n",
       " b'involved',\n",
       " b'killer',\n",
       " b'told',\n",
       " b'typical',\n",
       " b\"I'll\",\n",
       " b'guys',\n",
       " b'Film',\n",
       " b'throughout',\n",
       " b'scary',\n",
       " b'face',\n",
       " b're',\n",
       " b'head',\n",
       " b'turn',\n",
       " b'usual',\n",
       " b'cannot',\n",
       " b'mother',\n",
       " b'serious',\n",
       " b'hours',\n",
       " b'York',\n",
       " b'Its',\n",
       " b'ridiculous',\n",
       " b'bunch',\n",
       " b'feeling',\n",
       " b'next',\n",
       " b'dark',\n",
       " b\"Don't\",\n",
       " b'wants',\n",
       " b'care',\n",
       " b'Unfortunately',\n",
       " b'On',\n",
       " b'predictable',\n",
       " b'word',\n",
       " b'side',\n",
       " b'chance',\n",
       " b'tries',\n",
       " b'car',\n",
       " b'Then',\n",
       " b'fi',\n",
       " b'today',\n",
       " b'Disney',\n",
       " b\"There's\",\n",
       " b'remake',\n",
       " b'self',\n",
       " b'rented',\n",
       " b'sometimes',\n",
       " b'Peter',\n",
       " b'br',\n",
       " b'class',\n",
       " b'dead',\n",
       " b'strange',\n",
       " b'local',\n",
       " b'across',\n",
       " b'acted',\n",
       " b'girls',\n",
       " b'cool',\n",
       " b'animation',\n",
       " b'middle',\n",
       " b'happened',\n",
       " b'silly',\n",
       " b'rating',\n",
       " b'stop',\n",
       " b'instead',\n",
       " b'modern',\n",
       " b\"haven't\",\n",
       " b'interest',\n",
       " b'sound',\n",
       " b'murder',\n",
       " b'doubt',\n",
       " b'matter',\n",
       " b'complete',\n",
       " b'exactly',\n",
       " b'fantastic',\n",
       " b'experience',\n",
       " b\"won't\",\n",
       " b'release',\n",
       " b'talent',\n",
       " b'stuff',\n",
       " b'living',\n",
       " b'white',\n",
       " b'IMDb',\n",
       " b'single',\n",
       " b'obviously',\n",
       " b'Tom',\n",
       " b'upon',\n",
       " b'enjoyable',\n",
       " b'four',\n",
       " b'five',\n",
       " b'except',\n",
       " b'S',\n",
       " b'tried',\n",
       " b'bought',\n",
       " b'reading',\n",
       " b'highly',\n",
       " b'm',\n",
       " b'coming',\n",
       " b'due',\n",
       " b'behind',\n",
       " b'call',\n",
       " b'Most',\n",
       " b'e',\n",
       " b'brother',\n",
       " b'eyes',\n",
       " b'French',\n",
       " b'wonder',\n",
       " b'War',\n",
       " b'number',\n",
       " b'shown',\n",
       " b'sad',\n",
       " b'tells',\n",
       " b'famous',\n",
       " b'feature',\n",
       " b'filmed',\n",
       " b'strong',\n",
       " b'becomes',\n",
       " b'somewhat',\n",
       " b'box',\n",
       " b'Oh',\n",
       " b'hope',\n",
       " b'wrote',\n",
       " b'season',\n",
       " b'obvious',\n",
       " b'happen',\n",
       " b'adaptation',\n",
       " b'clich',\n",
       " b'Paul',\n",
       " b'lame',\n",
       " b'Like',\n",
       " b'viewing',\n",
       " b'poorly',\n",
       " b'moving',\n",
       " b'career',\n",
       " b'evil',\n",
       " b'recommend',\n",
       " b'King',\n",
       " b'husband',\n",
       " b'Oscar',\n",
       " b'named',\n",
       " b'viewer',\n",
       " b'begin',\n",
       " b'violence',\n",
       " b'write',\n",
       " b'non',\n",
       " b'ones',\n",
       " b'view',\n",
       " b'expectations',\n",
       " b'among',\n",
       " b'dull',\n",
       " b'wish',\n",
       " b'masterpiece',\n",
       " b'major',\n",
       " b'straight',\n",
       " b'agree',\n",
       " b'Movie',\n",
       " b'sit',\n",
       " b'hand',\n",
       " b'period',\n",
       " b'directing',\n",
       " b'particularly',\n",
       " b'country',\n",
       " b'fight',\n",
       " b'hate',\n",
       " b'daughter',\n",
       " b'happens',\n",
       " b'English',\n",
       " b'says',\n",
       " b'stand',\n",
       " b'turns',\n",
       " b'Christmas',\n",
       " b'f',\n",
       " b'attention',\n",
       " b\"That's\",\n",
       " b'taken',\n",
       " b'cheesy',\n",
       " b'comic',\n",
       " b'mostly',\n",
       " b\"wouldn't\",\n",
       " b'light',\n",
       " b'entertainment',\n",
       " b'order',\n",
       " b'roles',\n",
       " b'superb',\n",
       " b'dialog',\n",
       " b'easily',\n",
       " b'starring',\n",
       " b'Lee',\n",
       " b'weak',\n",
       " b'mystery',\n",
       " b'Night',\n",
       " b'seriously',\n",
       " b'realistic',\n",
       " b'reasons',\n",
       " b'theme',\n",
       " b'possibly',\n",
       " b'words',\n",
       " b'ten',\n",
       " b'cheap',\n",
       " b'similar',\n",
       " b'brought',\n",
       " b'co',\n",
       " b'including',\n",
       " b'nearly',\n",
       " b'cinematography',\n",
       " b'falls',\n",
       " b'basically',\n",
       " b'produced',\n",
       " b'talking',\n",
       " b'working',\n",
       " b'taking',\n",
       " b'reality',\n",
       " b'store',\n",
       " b'unique',\n",
       " b'forward',\n",
       " b'hell',\n",
       " b'alone',\n",
       " b'T',\n",
       " b'whose',\n",
       " b'sets',\n",
       " b'previous',\n",
       " b'soon',\n",
       " b'themselves',\n",
       " b'lack',\n",
       " b'killed',\n",
       " b'directors',\n",
       " b'c',\n",
       " b'bring',\n",
       " b'week',\n",
       " b'effort',\n",
       " b'average',\n",
       " b'ways',\n",
       " b'female',\n",
       " b'running',\n",
       " b'fast',\n",
       " b'editing',\n",
       " b'kept',\n",
       " b'caught',\n",
       " b'Japanese',\n",
       " b'popular',\n",
       " b'screenplay',\n",
       " b'elements',\n",
       " b'comedies',\n",
       " b'surprise',\n",
       " b'became',\n",
       " b'problems',\n",
       " b'Jack',\n",
       " b'features',\n",
       " b'annoying',\n",
       " b'animated',\n",
       " b'yes',\n",
       " b'd',\n",
       " b'possible',\n",
       " b'cover',\n",
       " b'follow',\n",
       " b'near',\n",
       " b'William',\n",
       " b'list',\n",
       " b'songs',\n",
       " b'cartoon',\n",
       " b'Having',\n",
       " b'voice',\n",
       " b'parents',\n",
       " b'Maybe',\n",
       " b'Star',\n",
       " b'events',\n",
       " b'none',\n",
       " b'body',\n",
       " b'crime',\n",
       " b'h',\n",
       " b'Great',\n",
       " b'Who',\n",
       " b'future',\n",
       " b'Very',\n",
       " b'weird',\n",
       " b'God',\n",
       " b\"aren't\",\n",
       " b'actual',\n",
       " b'despite',\n",
       " b'final',\n",
       " b'Is',\n",
       " b'gore',\n",
       " b'imagine',\n",
       " b'change',\n",
       " b'sister',\n",
       " b'cut',\n",
       " b\"you'll\",\n",
       " b'D',\n",
       " b'lots',\n",
       " b'beyond',\n",
       " b'moment',\n",
       " b'Dr',\n",
       " b'positive',\n",
       " b'level',\n",
       " b'fantasy',\n",
       " b'important',\n",
       " b'minute',\n",
       " b'sci',\n",
       " b'Joe',\n",
       " b'easy',\n",
       " b'era',\n",
       " b'House',\n",
       " b'awesome',\n",
       " b'romance',\n",
       " b'above',\n",
       " b'team',\n",
       " b'subject',\n",
       " b'rent',\n",
       " b'badly',\n",
       " b'happy',\n",
       " b'finds',\n",
       " b'points',\n",
       " b'rate',\n",
       " b\"'The\",\n",
       " b'begins',\n",
       " b'message',\n",
       " b'incredibly',\n",
       " b'concept',\n",
       " b'Italian',\n",
       " b'realize',\n",
       " b'Good',\n",
       " b'recent',\n",
       " b'score',\n",
       " b'By',\n",
       " b'casting',\n",
       " b'America',\n",
       " b'police',\n",
       " b'SPOILERS',\n",
       " b'power',\n",
       " b\"you've\",\n",
       " b'NOT',\n",
       " b\"they're\",\n",
       " b'giving',\n",
       " b'college',\n",
       " b'difficult',\n",
       " b'fall',\n",
       " b'laughed',\n",
       " b'knows',\n",
       " b'save',\n",
       " b'incredible',\n",
       " b'third',\n",
       " b'potential',\n",
       " b'leave',\n",
       " b'song',\n",
       " b'clearly',\n",
       " b'Scott',\n",
       " b'slasher',\n",
       " b'talk',\n",
       " b'laughing',\n",
       " b'Every',\n",
       " b'meets',\n",
       " b'move',\n",
       " b'fairly',\n",
       " b'kill',\n",
       " b'hero',\n",
       " b'plain',\n",
       " b'means',\n",
       " b'p',\n",
       " b'adventure',\n",
       " b'suspense',\n",
       " b'certain',\n",
       " b'joke',\n",
       " b'material',\n",
       " b'rated',\n",
       " b'showing',\n",
       " b'Also',\n",
       " b'mess',\n",
       " b'needs',\n",
       " b'World',\n",
       " b'Ben',\n",
       " b'gone',\n",
       " b'books',\n",
       " b'disappointment',\n",
       " b'cop',\n",
       " b'talented',\n",
       " b'copy',\n",
       " b'Bill',\n",
       " b'Festival',\n",
       " b'showed',\n",
       " b'success',\n",
       " b'interested',\n",
       " b'fails',\n",
       " b'shots',\n",
       " b'stage',\n",
       " b'describe',\n",
       " b'Wow',\n",
       " b'Okay',\n",
       " b\"she's\",\n",
       " b'wasted',\n",
       " b'Jane',\n",
       " b'particular',\n",
       " b'spent',\n",
       " b'whether',\n",
       " b'bored',\n",
       " b'wait',\n",
       " b'clear',\n",
       " b'rare',\n",
       " b'Jim',\n",
       " b'trash',\n",
       " b'consider',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b5d6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f11e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c123497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0e6a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cb596cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14f4ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e38f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 61s 74ms/step - loss: 0.6603 - accuracy: 0.5754\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 0.4916 - accuracy: 0.7484\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 60s 76ms/step - loss: 0.3460 - accuracy: 0.8517\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.2611 - accuracy: 0.8980\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.1863 - accuracy: 0.9326\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05969489",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5821ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=keras.layers.Input(shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2b6595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51ccb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edbe0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e4ab46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10465d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7537746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\saved_model.pb\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\assets\\tokens.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.data-00000-of-00001\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.index\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "850618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c40dc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbaf8c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5456 - accuracy: 0.7264\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5145 - accuracy: 0.7471\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5091 - accuracy: 0.7504\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5054 - accuracy: 0.7528\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5026 - accuracy: 0.7540\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deaf7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\mlbook\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41e5b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ec_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "dc_inputs = keras.layers.Input(shape=[None], dtype=np.int32) \n",
    "\n",
    "sequence_lenght = keras.layers.Input(shape=[], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d41674c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "\n",
    "ec_embeddigs = embeddings(ec_inputs)\n",
    "dc_embeddings = embeddings(dc_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f570e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoder = keras.layers.LSTM(512, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0f3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65a01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad24031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_string(grammar):\n",
    "    state=0\n",
    "    output=[]\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb54cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPTTTVPXVVE BTXSE BTSSSSSXXTTTVPXVVE BPTVPSE BTXXTTVVE BPVPSE BPTVVE BTXXVPXTVPXVPSE BPVVE BTXSE BTXXTVPXTVVE BTSXSE BPVVE BTSSXSE BTXXVVE BTSXXVVE BTSSSXXTTTTVVE BPTTTVVE BPTVVE BTXSE BPTVPXVPSE BTSXSE BTXSE BPTTTTTTVPXVPXTTVVE BPTTTVPXVPXTTVVE "
     ]
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3fd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "871000dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPBPVVEPE BTBTXXTTTTTTXPSETE BPBTXXTTTBVVEPE BPBPVPXVPXVESEPE BPVTSXSEPE BPETXSEPE BPBTXSESE ETBPVVETE BTBTXXTVPEVVETE BTBTSXXVPXTVTSETE STBTXSETE BTSTXSETE BPVTSXXVVEPE BPBTSSXSEEE BPBTXVEPE BPBTSSXSPPE BPBTSSXXTTVPSETE BPBTVXSEPE STBPTVPXVPSETE BPXTSSXSEPE BTBVXXVPXTTTTTTTTVPSETE BTBBVPSETE BTBPTTTTVVTTE BTBTXXTTTTTVPXVVEXE BPBTSSSSSSEPE "
     ]
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46f0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "835e6c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a01a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def generate_dataset(size):\n",
    "    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n",
    "                    for _ in range(size // 2)]\n",
    "    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "                   for _ in range(size - size // 2)]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
    "                 [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7676971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2859f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "338eee15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15,), dtype=int32, numpy=array([0, 4, 0, 2, 4, 5, 2, 6, 4, 5, 2, 3, 1, 4, 1])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f639913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a580a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "from tensorflow import keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3623c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e15f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f8cba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3365 - accuracy: 0.8646 - val_loss: 0.4592 - val_accuracy: 0.7175\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2053 - accuracy: 0.9321 - val_loss: 0.1129 - val_accuracy: 0.9735\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1182 - accuracy: 0.9650 - val_loss: 0.0844 - val_accuracy: 0.9805\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0918 - accuracy: 0.9774 - val_loss: 0.1764 - val_accuracy: 0.9480\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1926 - accuracy: 0.9303 - val_loss: 0.1040 - val_accuracy: 0.9745\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0885 - accuracy: 0.9786 - val_loss: 0.0849 - val_accuracy: 0.9790\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0800 - accuracy: 0.9805 - val_loss: 0.0704 - val_accuracy: 0.9815\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0455 - accuracy: 0.9873 - val_loss: 0.0148 - val_accuracy: 0.9980\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 8.1170e-04 - accuracy: 1.0000 - val_loss: 7.2789e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.9744e-04 - accuracy: 1.0000 - val_loss: 4.6742e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.5722e-04 - accuracy: 1.0000 - val_loss: 3.4646e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.7941e-04 - accuracy: 1.0000 - val_loss: 2.9199e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 2.3171e-04 - accuracy: 1.0000 - val_loss: 2.3501e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.9627e-04 - accuracy: 1.0000 - val_loss: 2.0534e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.7192e-04 - accuracy: 1.0000 - val_loss: 1.7827e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5274e-04 - accuracy: 1.0000 - val_loss: 1.5872e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.3747e-04 - accuracy: 1.0000 - val_loss: 1.4429e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2474e-04 - accuracy: 1.0000 - val_loss: 1.3151e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.1442e-04 - accuracy: 1.0000 - val_loss: 1.2078e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d762b7fa0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09ae3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1b3b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b76dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "y_proba = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bfd0e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001542 ],\n",
       "       [0.9998921]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d5c2e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.15%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.99%\n"
     ]
    }
   ],
   "source": [
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6644c92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "November 25, 1916\n",
      "March 27, 2020\n",
      "April 01, 1953\n",
      "September 21, 1928\n",
      "May 14, 1942\n",
      "October 25, 1952\n",
      "February 22, 2022\n",
      "June 08, 1919\n",
      "April 15, 1982\n",
      "August 11, 2015\n",
      "April 08, 2011\n",
      "August 01, 2019\n",
      "February 11, 2002\n",
      "April 17, 1949\n",
      "February 15, 1908\n",
      "December 31, 1996\n",
      "['J', 'a', 'n', 'u', 'r', 'y', 'F', 'e', 'b', 'M', 'c', 'h', 'A', 'p', 'i', 'l', 'g', 's', 't', 'S', 'm', 'O', 'o', 'N', 'v', 'D']\n",
      "[11, 52, 54, 45, 48, 63, 2, 2, 64, 63, 2, 0, 1, 9]\n",
      "[11, 52, 54, 45, 48, 63, 2, 1, 64, 63, 2, 0, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "import calendar\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "def generate_random_date():\n",
    "    # Gera um ano aleatrio\n",
    "    year = random.randint(1900, 2024)\n",
    "    \n",
    "    # Gera um ms aleatrio\n",
    "    month = random.randint(1, 12)\n",
    "    \n",
    "    # Obtm o nmero de dias do ms e ano gerado\n",
    "    _, num_days = calendar.monthrange(year, month)\n",
    "    \n",
    "    # Gera um dia aleatrio dentro do intervalo vlido para o ms\n",
    "    day = random.randint(1, num_days)\n",
    "    \n",
    "    # Cria a data\n",
    "    random_date = date(year, month, day)\n",
    "    \n",
    "    # Converte a data para o formato desejado\n",
    "    date_str = random_date.strftime(\"%B %d, %Y\")\n",
    "    return date_str\n",
    "\n",
    "# Teste\n",
    "for _ in range(5):\n",
    "    print(generate_random_date())\n",
    "\n",
    "\n",
    "test = generate_random_date()\n",
    "\n",
    "print(test)\n",
    "\n",
    "from datetime import datetime\n",
    "def convert_format(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%B %d, %Y')\n",
    "    return date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "examples = []\n",
    "for _ in range(10):\n",
    "    dates = generate_random_date()\n",
    "    print(dates)\n",
    "    new_date = convert_format(dates)\n",
    "    examples.append([dates, new_date])\n",
    "\n",
    "examples\n",
    "\n",
    "def create_dataset(n_samples):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(n_samples):\n",
    "        date_str = generate_random_date()\n",
    "        X.append(date_str)\n",
    "        target = convert_format(date_str)\n",
    "        y.append(target)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "\n",
    "n_samples = 100\n",
    "test1, test1_y = create_dataset(n_samples)\n",
    "test1\n",
    "\n",
    "vocab = []\n",
    "for month in MONTHS:\n",
    "    for char in month:\n",
    "        if char not in vocab:\n",
    "            vocab.append(char)\n",
    "print(vocab)\n",
    "\n",
    "for i in range(10):\n",
    "    vocab.append(str(i))\n",
    "\n",
    "INPUT_CHARS = \"0123456789-ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ,\"\n",
    "from tensorflow import keras\n",
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]\n",
    "\n",
    "# Exemplo com uma string de data\n",
    "date_str = \"April 22, 2019\"\n",
    "date_ids = date_str_to_ids(date_str, INPUT_CHARS)\n",
    "print(date_ids)\n",
    "\n",
    "\n",
    "date_str = \"April 21, 2019\"\n",
    "date_ids = date_str_to_ids(date_str, INPUT_CHARS)\n",
    "print(date_ids)\n",
    "\n",
    "\n",
    "X_train, y_train = create_dataset(10000)\n",
    "X_test, y_test = create_dataset(2000)\n",
    "X_val, y_val = create_dataset(2000)\n",
    "\n",
    "import numpy as np\n",
    "y_train = np.array(y_train)\n",
    "max_output_length = len(y_train[0])\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Corrigir: transformar para listas de IDs\n",
    "X_train_ids = [date_str_to_ids(s, INPUT_CHARS) for s in X_train]\n",
    "X_val_ids   = [date_str_to_ids(s, INPUT_CHARS) for s in X_val]\n",
    "y_train_ids = [date_str_to_ids(s, INPUT_CHARS) for s in y_train]\n",
    "y_val_ids   = [date_str_to_ids(s, INPUT_CHARS) for s in y_val]\n",
    "\n",
    "# Criar RaggedTensor para entrada\n",
    "X_train = tf.ragged.constant(X_train_ids, dtype=tf.int32)\n",
    "X_val   = tf.ragged.constant(X_val_ids, dtype=tf.int32)\n",
    "\n",
    "# Sada como tensor denso (mesmo tamanho sempre no formato YYYY-MM-DD)\n",
    "y_train = tf.constant(y_train_ids, dtype=tf.int32)\n",
    "y_val   = tf.constant(y_val_ids, dtype=tf.int32)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(None,), ragged=True, dtype=\"int32\"),\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1, output_dim=100),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(INPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83f5be89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\mlbook\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_12/sequential_10/lstm_7/RaggedToTensor/boolean_mask_1/GatherV2:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_12/sequential_10/lstm_7/RaggedToTensor/boolean_mask/GatherV2:0\", shape=(None, 100), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_12/sequential_10/lstm_7/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 21ms/step - loss: 1.7737 - accuracy: 0.4021 - val_loss: 1.2421 - val_accuracy: 0.5180\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 1.0835 - accuracy: 0.6056 - val_loss: 0.8953 - val_accuracy: 0.6711\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.6861 - accuracy: 0.7673 - val_loss: 0.4850 - val_accuracy: 0.8414\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.3356 - accuracy: 0.9088 - val_loss: 0.2136 - val_accuracy: 0.9498\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.1202 - accuracy: 0.9840 - val_loss: 0.0638 - val_accuracy: 0.9979\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0391 - accuracy: 0.9996 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 6s 20ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 6s 18ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 9.2820e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 7.9719e-04 - accuracy: 1.0000 - val_loss: 7.4007e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 6.3757e-04 - accuracy: 1.0000 - val_loss: 5.9595e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 5.1404e-04 - accuracy: 1.0000 - val_loss: 4.8488e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 4.1777e-04 - accuracy: 1.0000 - val_loss: 3.9552e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 6s 19ms/step - loss: 3.4113e-04 - accuracy: 1.0000 - val_loss: 3.2325e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "838741ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 516ms/step\n"
     ]
    }
   ],
   "source": [
    "X_new = date_str_to_ids(\"September 17, 2009\", INPUT_CHARS)\n",
    "X_new_ragged = tf.ragged.constant([X_new], dtype=tf.int32)\n",
    "\n",
    "\n",
    "pred = model.predict(X_new_ragged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8614c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[6.06561662e-06 2.65691033e-05 9.99958277e-01 1.20932066e-06\n",
      "   1.89052869e-07 2.67575351e-10 1.53370691e-10 2.22395222e-08\n",
      "   9.67464331e-08 8.31002112e-08 2.19541832e-07 1.14669433e-07\n",
      "   2.28936884e-07 9.69453708e-08 2.13713619e-07 1.58562855e-07\n",
      "   1.19060417e-07 9.03764743e-08 1.25418723e-07 1.05091594e-07\n",
      "   9.72747003e-08 8.40092440e-08 1.45499087e-07 1.36460883e-07\n",
      "   6.72591369e-08 1.94589944e-07 1.34151037e-07 7.36523376e-08\n",
      "   1.79632949e-07 1.73990529e-07 6.45965486e-08 7.46202389e-08\n",
      "   1.43877628e-07 1.53673696e-07 1.62878990e-07 1.50736440e-07\n",
      "   1.46256028e-07 2.25131686e-07 1.64559935e-07 5.63306060e-08\n",
      "   7.08515770e-08 1.12026143e-07 3.04462617e-07 2.60902937e-07\n",
      "   1.15477462e-07 1.41858564e-07 1.92993141e-07 7.83968304e-08\n",
      "   1.16081651e-07 1.94005878e-07 1.57486241e-07 1.34645020e-07\n",
      "   6.47797691e-08 1.97530284e-07 4.86967338e-08 2.12709125e-07\n",
      "   1.26040419e-07 1.06016692e-07 9.88551818e-08 7.23001392e-08\n",
      "   7.15687136e-08 1.00324442e-07 1.89590907e-07 9.97680019e-08\n",
      "   6.28614742e-08 1.20979649e-07]\n",
      "  [9.99982953e-01 1.01980673e-08 6.12277802e-07 7.87726265e-07\n",
      "   2.24513741e-09 5.34860045e-10 2.86872327e-12 3.79193166e-09\n",
      "   1.75150016e-11 1.56376627e-05 3.63279268e-10 7.24742932e-10\n",
      "   3.02752512e-09 2.82907209e-09 3.15900284e-09 1.60669544e-09\n",
      "   1.93766403e-09 1.30900768e-09 8.26951840e-10 5.67768998e-10\n",
      "   4.75532236e-09 1.61277158e-09 2.76446466e-09 1.51001300e-09\n",
      "   1.92613059e-09 3.32996630e-09 2.69348122e-09 1.27667155e-09\n",
      "   1.42115286e-09 1.47124313e-09 1.35555633e-09 9.70183378e-10\n",
      "   1.89377847e-09 1.67017011e-09 2.44323095e-09 2.05316919e-09\n",
      "   2.75895462e-09 3.58775343e-09 1.62434233e-09 6.58649912e-10\n",
      "   1.12446463e-09 1.12419440e-09 2.85748913e-09 7.70354114e-10\n",
      "   1.01688891e-09 2.04184381e-09 2.37322917e-09 9.10013231e-10\n",
      "   2.08682294e-09 2.45538212e-09 1.09910170e-09 2.55602450e-09\n",
      "   8.78712159e-10 1.01337072e-09 6.56800780e-10 1.53863866e-09\n",
      "   1.00092101e-09 2.00902273e-09 2.09215933e-09 5.14204068e-10\n",
      "   1.96263339e-09 1.94330463e-09 3.78039910e-09 1.98860350e-09\n",
      "   3.21508931e-10 1.05964215e-09]\n",
      "  [9.99946237e-01 3.92023976e-05 1.55374562e-06 6.91231298e-06\n",
      "   7.60472219e-09 4.08337630e-09 2.28443944e-10 2.04979251e-08\n",
      "   1.02441495e-08 6.07391985e-06 1.39861500e-09 4.68984296e-10\n",
      "   2.09657247e-09 2.41285947e-09 2.26838881e-09 9.59388458e-10\n",
      "   1.08570186e-09 5.11662102e-10 1.50314450e-09 3.45125289e-10\n",
      "   4.01895761e-09 7.40413952e-10 3.99522637e-09 7.97874045e-10\n",
      "   3.12294834e-09 1.13326182e-09 1.38987388e-09 6.85309254e-10\n",
      "   4.17928331e-10 3.84473786e-09 2.40464815e-10 5.71186987e-10\n",
      "   1.95134131e-09 1.31729228e-09 3.02986880e-09 1.51476909e-09\n",
      "   2.73996448e-09 2.25656049e-09 1.05339271e-09 2.29058816e-09\n",
      "   1.04859765e-09 1.15689469e-09 1.88293625e-09 7.67187980e-10\n",
      "   2.34358022e-09 2.99708414e-09 1.22115784e-09 4.58317745e-10\n",
      "   1.29976208e-09 2.41072939e-09 1.37624356e-09 1.41049072e-09\n",
      "   5.63799618e-10 1.63202762e-09 5.28291799e-10 1.16565702e-09\n",
      "   1.24989497e-09 9.31173860e-10 1.83887250e-09 6.77696066e-10\n",
      "   8.13275558e-10 1.81305182e-09 1.11224374e-09 2.97918801e-09\n",
      "   4.14264151e-10 1.33743183e-09]\n",
      "  [5.64205075e-05 8.43866437e-05 6.39030873e-08 2.92700988e-05\n",
      "   2.47947128e-06 4.32515153e-05 1.48701620e-05 2.08265796e-08\n",
      "   3.80076854e-05 9.99728024e-01 3.07856840e-06 2.84467894e-09\n",
      "   5.48462165e-09 4.16308454e-09 5.06692466e-09 2.39914533e-09\n",
      "   3.08280290e-09 2.79487722e-09 2.92921842e-09 9.18770671e-10\n",
      "   6.10233775e-09 3.57721897e-09 2.93421176e-09 2.18059171e-09\n",
      "   8.13126455e-09 3.65590203e-09 2.67670530e-09 2.08738959e-09\n",
      "   1.59326896e-09 1.50034847e-08 1.22999289e-09 7.89015742e-10\n",
      "   2.87216761e-09 5.73701264e-09 2.51161292e-09 4.02117051e-09\n",
      "   4.16272705e-09 3.03353254e-09 2.81756063e-09 1.51659290e-08\n",
      "   3.50850438e-09 5.79004311e-09 6.21134744e-09 4.29530056e-09\n",
      "   2.79465850e-09 6.59797372e-09 1.85932669e-09 2.28042496e-09\n",
      "   4.17397672e-09 3.13463056e-09 5.70591174e-09 3.68088071e-09\n",
      "   1.97258743e-09 1.81578785e-09 1.73619319e-09 2.89966762e-09\n",
      "   2.72508216e-09 2.78437917e-09 1.08912577e-08 3.80344423e-09\n",
      "   2.08140616e-09 2.83908497e-09 3.17720539e-09 2.85408630e-09\n",
      "   1.51631141e-09 6.13223028e-09]\n",
      "  [1.26566604e-07 8.46380033e-07 7.42277972e-09 5.89943966e-07\n",
      "   2.21457808e-09 5.18738474e-09 3.10026103e-08 2.22971330e-09\n",
      "   8.32918516e-08 5.46759622e-07 9.99997735e-01 3.20672794e-10\n",
      "   5.90774707e-10 2.30293173e-10 5.01409803e-10 3.16974336e-10\n",
      "   1.36480383e-10 4.27874319e-10 3.41284084e-10 1.41819181e-10\n",
      "   5.19095489e-10 7.08086312e-10 6.63278821e-10 2.14889537e-10\n",
      "   3.87209709e-10 1.58714569e-10 4.20964458e-10 3.89496324e-10\n",
      "   1.98304886e-10 5.76453496e-10 6.26397961e-11 7.85553220e-11\n",
      "   3.06760062e-10 4.63856481e-10 2.48033177e-10 4.93292629e-10\n",
      "   1.70757547e-10 4.69378925e-10 1.73376466e-10 3.43383322e-10\n",
      "   4.61537225e-10 5.85703375e-10 5.07259290e-10 4.14634882e-10\n",
      "   2.54172378e-10 4.96555075e-10 4.55794014e-10 1.36270731e-10\n",
      "   2.31324196e-10 1.95707284e-10 4.28034302e-10 4.79507378e-10\n",
      "   4.07227502e-10 1.41599441e-10 1.99956288e-10 1.95945593e-10\n",
      "   3.39566014e-10 2.31304351e-10 3.37090605e-10 2.36369480e-10\n",
      "   4.74274564e-10 3.54701324e-10 9.91439375e-10 1.91945765e-10\n",
      "   1.19170021e-10 1.01039421e-09]\n",
      "  [9.99945760e-01 5.31607293e-05 3.93140539e-07 2.12500268e-07\n",
      "   5.18654841e-10 3.65554895e-12 2.41711989e-12 7.53230900e-09\n",
      "   3.19622356e-10 2.60506852e-07 1.66680806e-07 2.30137940e-11\n",
      "   6.69655720e-11 2.99069519e-11 1.03887378e-10 8.00999198e-11\n",
      "   2.56151454e-11 3.47477741e-11 4.89026805e-11 2.24781947e-11\n",
      "   1.41317694e-10 4.63020698e-11 3.07393055e-10 5.26275828e-11\n",
      "   9.13695578e-11 1.65810837e-11 7.76076634e-11 4.87759902e-11\n",
      "   3.67659826e-11 1.08337977e-10 2.21049828e-11 2.18015779e-11\n",
      "   9.17023263e-11 8.04732600e-11 4.24758805e-11 6.35335465e-11\n",
      "   4.98749063e-11 5.17325696e-11 2.65335167e-11 9.92851912e-11\n",
      "   6.33171293e-11 3.84983156e-11 3.12011562e-11 3.10390533e-11\n",
      "   6.16927481e-11 1.12100454e-10 2.71137227e-11 4.20303757e-11\n",
      "   9.97966709e-11 3.19284599e-11 5.86324253e-11 1.71476486e-10\n",
      "   6.39620301e-11 4.64608699e-11 2.12476148e-11 4.51592305e-11\n",
      "   2.54233717e-11 7.32588781e-11 1.00940506e-10 2.23755754e-11\n",
      "   3.01232407e-11 1.00451703e-10 4.86583551e-11 1.55270186e-10\n",
      "   3.03024585e-11 1.32438283e-10]\n",
      "  [5.44999028e-04 1.41815294e-03 2.98451905e-05 1.20950172e-04\n",
      "   5.32179780e-04 3.27355119e-05 7.20508688e-05 3.17362465e-05\n",
      "   2.40711161e-04 9.96920705e-01 5.19466012e-05 5.23589847e-08\n",
      "   5.19688648e-08 3.99449895e-08 1.24021497e-07 4.63494594e-08\n",
      "   7.43151389e-08 3.66528532e-08 7.34974108e-08 3.38272379e-08\n",
      "   1.22062403e-07 4.13185255e-08 1.03664561e-07 7.22228108e-08\n",
      "   1.38468181e-07 7.28980467e-08 2.99717300e-08 4.90763199e-08\n",
      "   6.32580210e-08 2.51865885e-07 7.42822621e-08 2.75350978e-08\n",
      "   4.76385829e-08 1.38276576e-07 4.49886883e-08 6.92629882e-08\n",
      "   5.48543717e-08 5.91986975e-08 4.82981442e-08 2.39257048e-07\n",
      "   5.67891938e-08 8.57362963e-08 7.90091335e-08 8.92237608e-08\n",
      "   7.03099801e-08 1.17612394e-07 2.84408177e-08 5.76522545e-08\n",
      "   4.08812610e-08 9.20555010e-08 8.64563674e-08 6.65383268e-08\n",
      "   3.85983334e-08 4.25758344e-08 2.40137457e-08 4.91216490e-08\n",
      "   7.57515366e-08 6.56234249e-08 8.46466506e-08 4.26905729e-08\n",
      "   4.64767567e-08 6.15139513e-08 3.69039910e-08 6.93125415e-08\n",
      "   5.14401783e-08 7.49104316e-08]\n",
      "  [1.04101539e-09 1.32757492e-07 1.68696047e-07 1.12893167e-06\n",
      "   1.64216701e-07 6.99412084e-08 8.97523023e-07 8.25166182e-07\n",
      "   2.31935843e-07 8.56540581e-08 9.99996066e-01 2.23882513e-09\n",
      "   4.04612477e-09 2.11239204e-09 4.75569317e-09 5.94446314e-09\n",
      "   1.86026872e-09 3.01507708e-09 5.60124613e-09 2.62467048e-09\n",
      "   2.78107559e-09 3.64226382e-09 1.07680780e-08 2.61665822e-09\n",
      "   1.70092929e-09 2.44274601e-09 2.73888845e-09 4.15290557e-09\n",
      "   5.36301537e-09 3.52815688e-09 1.47047696e-09 1.65557046e-09\n",
      "   2.36363351e-09 2.57992094e-09 2.93036129e-09 4.00745392e-09\n",
      "   4.25390301e-09 7.52687157e-09 1.50410273e-09 2.53229948e-09\n",
      "   1.94295224e-09 7.40771844e-09 4.69850381e-09 3.77213683e-09\n",
      "   4.37980452e-09 2.80052026e-09 2.76969425e-09 2.11067652e-09\n",
      "   3.07982040e-09 4.74695705e-09 3.66795416e-09 2.47308685e-09\n",
      "   2.37173059e-09 1.71400172e-09 7.04570291e-10 1.49253143e-09\n",
      "   8.07042699e-09 2.32321473e-09 3.33232930e-09 3.61815444e-09\n",
      "   6.28034114e-09 4.32723501e-09 8.15617263e-09 1.25867516e-09\n",
      "   3.79946030e-09 3.84759957e-09]\n",
      "  [7.63209100e-05 9.99266446e-01 4.51245374e-04 1.85143581e-04\n",
      "   1.29718307e-08 1.26118185e-10 1.03221953e-09 1.03427319e-05\n",
      "   8.03690270e-08 8.84740359e-07 8.54736936e-06 5.02180919e-09\n",
      "   1.53166830e-08 9.76237402e-09 1.62009552e-08 1.27903936e-08\n",
      "   1.84423179e-08 5.69114977e-09 1.15788081e-08 1.24970354e-08\n",
      "   2.34352502e-08 8.79895445e-09 1.16610167e-07 3.48185267e-08\n",
      "   2.38579840e-08 9.40770661e-09 1.00948441e-08 1.71986905e-08\n",
      "   1.38498004e-08 2.07386961e-08 6.03515460e-09 5.36381650e-09\n",
      "   1.89238651e-08 1.10330554e-08 1.28872921e-08 1.75229573e-08\n",
      "   1.77495423e-08 1.97337826e-08 4.53916371e-09 5.10764053e-08\n",
      "   5.40765921e-09 3.95839841e-08 1.42438337e-08 1.77940134e-08\n",
      "   2.13756355e-08 1.74458208e-08 9.99992622e-09 1.75374684e-08\n",
      "   2.41800855e-08 8.32328073e-09 2.15170459e-08 2.57963837e-08\n",
      "   1.26583428e-08 1.06282503e-08 2.58428612e-09 1.44549848e-08\n",
      "   3.19367182e-08 1.44730823e-08 1.50404134e-08 1.15013297e-08\n",
      "   1.29679716e-08 4.96575829e-08 1.56415840e-08 9.37237044e-09\n",
      "   9.34284916e-09 2.93186861e-08]\n",
      "  [6.13645592e-04 1.32896721e-05 9.13830081e-06 3.15108191e-04\n",
      "   1.92084277e-04 1.68659142e-04 2.71795579e-05 9.98631299e-01\n",
      "   8.93995718e-07 2.19968060e-05 3.61847015e-06 3.70215041e-08\n",
      "   5.70883749e-08 2.61349182e-08 5.65960505e-08 3.09483212e-08\n",
      "   9.60491775e-08 1.93200336e-08 2.29379449e-08 3.95071744e-08\n",
      "   8.00469451e-08 4.18277395e-08 1.91401909e-07 8.69737065e-08\n",
      "   7.30530871e-08 5.98616623e-08 2.28775150e-08 6.26696703e-08\n",
      "   7.05288983e-08 3.95881870e-08 6.01514571e-08 4.21969801e-08\n",
      "   2.78874168e-08 4.78023132e-08 5.18004164e-08 8.67745342e-08\n",
      "   2.76912484e-08 8.89401548e-08 3.22885754e-08 1.14128689e-07\n",
      "   2.19715695e-08 9.81101707e-08 7.96207402e-08 7.95899169e-08\n",
      "   4.40238637e-08 4.80826792e-08 2.71931793e-08 5.65349865e-08\n",
      "   7.12066353e-08 4.73669388e-08 6.15800246e-08 8.84242581e-08\n",
      "   1.77899651e-08 5.19371426e-08 2.88347941e-08 3.35993384e-08\n",
      "   9.60255448e-08 8.23720612e-08 3.97973672e-08 2.31019790e-08\n",
      "   3.80160756e-08 5.27125863e-08 3.21855822e-08 3.14933573e-08\n",
      "   3.70387347e-08 7.51498277e-08]]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9388f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n"
     ]
    }
   ],
   "source": [
    "ids = np.argmax(pred, axis=-1)\n",
    "\n",
    "# Funo para converter IDs para string\n",
    "def ids_to_date_strs(ids_array, chars=INPUT_CHARS):\n",
    "    return [\"\".join([chars[i] for i in row]) for row in ids_array]\n",
    "\n",
    "# Converta para string e imprima\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb8550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
