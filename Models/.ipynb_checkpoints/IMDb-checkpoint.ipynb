{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4ffe268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2419462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17464789/17464789 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f7f06b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7363fab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1641221/1641221 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfd1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {id_ + 3: word for word, id_ in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d54b4f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eaf175b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> this film was just brilliant casting location scenery story'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac873be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fcd13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd5cf97e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset.\n",
       "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_path='C:\\\\Users\\\\User\\\\tensorflow_datasets\\\\imdb_reviews\\\\plain_text\\\\1.0.0',\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94bf3939",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc61cfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b22a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5de993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a59af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa8a96c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "279412af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6b099f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e44bee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'<pad>',\n",
       " b'the',\n",
       " b'a',\n",
       " b'of',\n",
       " b'and',\n",
       " b'to',\n",
       " b'I',\n",
       " b'is',\n",
       " b'in',\n",
       " b'this',\n",
       " b'it',\n",
       " b'was',\n",
       " b'movie',\n",
       " b'that',\n",
       " b'The',\n",
       " b'film',\n",
       " b'with',\n",
       " b'for',\n",
       " b'as',\n",
       " b'on',\n",
       " b'but',\n",
       " b'have',\n",
       " b'This',\n",
       " b'one',\n",
       " b'not',\n",
       " b'be',\n",
       " b'are',\n",
       " b'you',\n",
       " b'an',\n",
       " b'at',\n",
       " b'about',\n",
       " b'by',\n",
       " b'all',\n",
       " b'his',\n",
       " b'so',\n",
       " b'like',\n",
       " b'from',\n",
       " b'who',\n",
       " b'has',\n",
       " b'It',\n",
       " b'good',\n",
       " b'my',\n",
       " b'just',\n",
       " b'very',\n",
       " b'out',\n",
       " b'or',\n",
       " b'story',\n",
       " b'some',\n",
       " b'time',\n",
       " b'had',\n",
       " b'he',\n",
       " b'they',\n",
       " b'really',\n",
       " b'me',\n",
       " b'when',\n",
       " b'what',\n",
       " b'first',\n",
       " b'movies',\n",
       " b'bad',\n",
       " b'see',\n",
       " b'seen',\n",
       " b'up',\n",
       " b'only',\n",
       " b'were',\n",
       " b\"it's\",\n",
       " b'would',\n",
       " b'more',\n",
       " b'made',\n",
       " b'great',\n",
       " b'can',\n",
       " b'been',\n",
       " b'i',\n",
       " b'her',\n",
       " b'no',\n",
       " b'A',\n",
       " b'which',\n",
       " b'even',\n",
       " b'films',\n",
       " b'there',\n",
       " b'ever',\n",
       " b'people',\n",
       " b'much',\n",
       " b'because',\n",
       " b'most',\n",
       " b'plot',\n",
       " b'if',\n",
       " b'than',\n",
       " b'acting',\n",
       " b'get',\n",
       " b'their',\n",
       " b'well',\n",
       " b'into',\n",
       " b'how',\n",
       " b'best',\n",
       " b'think',\n",
       " b'other',\n",
       " b'its',\n",
       " b\"It's\",\n",
       " b'saw',\n",
       " b'could',\n",
       " b'watch',\n",
       " b'many',\n",
       " b\"don't\",\n",
       " b'do',\n",
       " b'will',\n",
       " b'say',\n",
       " b'show',\n",
       " b'love',\n",
       " b'make',\n",
       " b'two',\n",
       " b'way',\n",
       " b'know',\n",
       " b'years',\n",
       " b'after',\n",
       " b'But',\n",
       " b'we',\n",
       " b'watching',\n",
       " b'In',\n",
       " b'never',\n",
       " b'any',\n",
       " b'life',\n",
       " b'characters',\n",
       " b\"I'm\",\n",
       " b'too',\n",
       " b'did',\n",
       " b\"I've\",\n",
       " b'little',\n",
       " b'over',\n",
       " b'If',\n",
       " b'she',\n",
       " b'then',\n",
       " b'being',\n",
       " b'thought',\n",
       " b'them',\n",
       " b'off',\n",
       " b'There',\n",
       " b's',\n",
       " b'funny',\n",
       " b'better',\n",
       " b'also',\n",
       " b'watched',\n",
       " b'where',\n",
       " b'still',\n",
       " b'worst',\n",
       " b'old',\n",
       " b'am',\n",
       " b'through',\n",
       " b'actors',\n",
       " b'And',\n",
       " b'character',\n",
       " b'such',\n",
       " b\"didn't\",\n",
       " b'back',\n",
       " b'got',\n",
       " b'horror',\n",
       " b'those',\n",
       " b'series',\n",
       " b'lot',\n",
       " b'should',\n",
       " b'comedy',\n",
       " b'go',\n",
       " b'your',\n",
       " b'few',\n",
       " b'something',\n",
       " b'does',\n",
       " b'thing',\n",
       " b'him',\n",
       " b'man',\n",
       " b'What',\n",
       " b'going',\n",
       " b'original',\n",
       " b'director',\n",
       " b'work',\n",
       " b'here',\n",
       " b'before',\n",
       " b'real',\n",
       " b'actually',\n",
       " b'cast',\n",
       " b'pretty',\n",
       " b'TV',\n",
       " b'quite',\n",
       " b'why',\n",
       " b'every',\n",
       " b'He',\n",
       " b'find',\n",
       " b'As',\n",
       " b'scenes',\n",
       " b'young',\n",
       " b\"can't\",\n",
       " b'interesting',\n",
       " b'same',\n",
       " b'fan',\n",
       " b'now',\n",
       " b'makes',\n",
       " b'end',\n",
       " b'big',\n",
       " b'When',\n",
       " b'long',\n",
       " b'nothing',\n",
       " b'times',\n",
       " b'found',\n",
       " b\"'s\",\n",
       " b'must',\n",
       " b'these',\n",
       " b'again',\n",
       " b'another',\n",
       " b'part',\n",
       " b'action',\n",
       " b'though',\n",
       " b'always',\n",
       " b'script',\n",
       " b\"doesn't\",\n",
       " b'last',\n",
       " b'new',\n",
       " b'fact',\n",
       " b'year',\n",
       " b'while',\n",
       " b'around',\n",
       " b'look',\n",
       " b'since',\n",
       " b't',\n",
       " b'minutes',\n",
       " b'family',\n",
       " b'DVD',\n",
       " b'down',\n",
       " b'scene',\n",
       " b'things',\n",
       " b'want',\n",
       " b'read',\n",
       " b'seems',\n",
       " b'My',\n",
       " b'fun',\n",
       " b'may',\n",
       " b'world',\n",
       " b'done',\n",
       " b'far',\n",
       " b'come',\n",
       " b'right',\n",
       " b'take',\n",
       " b\"wasn't\",\n",
       " b'You',\n",
       " b'th',\n",
       " b'believe',\n",
       " b'almost',\n",
       " b'enough',\n",
       " b'hard',\n",
       " b'whole',\n",
       " b'anything',\n",
       " b'book',\n",
       " b\"isn't\",\n",
       " b'probably',\n",
       " b'least',\n",
       " b'give',\n",
       " b'bit',\n",
       " b'They',\n",
       " b'American',\n",
       " b'seeing',\n",
       " b'without',\n",
       " b'feel',\n",
       " b'One',\n",
       " b'awful',\n",
       " b'idea',\n",
       " b'making',\n",
       " b'between',\n",
       " b'remember',\n",
       " b'came',\n",
       " b'music',\n",
       " b'kind',\n",
       " b'set',\n",
       " b'excellent',\n",
       " b'John',\n",
       " b'classic',\n",
       " b'us',\n",
       " b'both',\n",
       " b'Well',\n",
       " b'version',\n",
       " b'performance',\n",
       " b'gets',\n",
       " b'plays',\n",
       " b'day',\n",
       " b'Not',\n",
       " b'low',\n",
       " b'might',\n",
       " b'own',\n",
       " b'loved',\n",
       " b'budget',\n",
       " b'role',\n",
       " b'reason',\n",
       " b'high',\n",
       " b'First',\n",
       " b'girl',\n",
       " b'shows',\n",
       " b'looking',\n",
       " b'After',\n",
       " b'For',\n",
       " b'So',\n",
       " b'video',\n",
       " b'special',\n",
       " b'However',\n",
       " b'effects',\n",
       " b'woman',\n",
       " b'different',\n",
       " b'terrible',\n",
       " b'true',\n",
       " b'point',\n",
       " b'start',\n",
       " b'actor',\n",
       " b'put',\n",
       " b'night',\n",
       " b'three',\n",
       " b'played',\n",
       " b'liked',\n",
       " b'yet',\n",
       " b'boring',\n",
       " b'beautiful',\n",
       " b'takes',\n",
       " b'especially',\n",
       " b'based',\n",
       " b'ago',\n",
       " b'main',\n",
       " b'short',\n",
       " b'play',\n",
       " b'guy',\n",
       " b'title',\n",
       " b'said',\n",
       " b'enjoyed',\n",
       " b'anyone',\n",
       " b'wonderful',\n",
       " b'trying',\n",
       " b\"that's\",\n",
       " b'sure',\n",
       " b'went',\n",
       " b'favorite',\n",
       " b'having',\n",
       " b'stars',\n",
       " b'together',\n",
       " b'Hollywood',\n",
       " b'absolutely',\n",
       " b'screen',\n",
       " b'our',\n",
       " b'wrong',\n",
       " b'rather',\n",
       " b'half',\n",
       " b'production',\n",
       " b'episode',\n",
       " b'goes',\n",
       " b'place',\n",
       " b'comes',\n",
       " b'early',\n",
       " b'piece',\n",
       " b'written',\n",
       " b'We',\n",
       " b'heard',\n",
       " b'away',\n",
       " b\"couldn't\",\n",
       " b'second',\n",
       " b'money',\n",
       " b'during',\n",
       " b'friends',\n",
       " b'job',\n",
       " b'THE',\n",
       " b'someone',\n",
       " b'entertaining',\n",
       " b'sense',\n",
       " b'looks',\n",
       " b'She',\n",
       " b'line',\n",
       " b'school',\n",
       " b'drama',\n",
       " b'each',\n",
       " b'flick',\n",
       " b'everything',\n",
       " b'understand',\n",
       " b'No',\n",
       " b'mean',\n",
       " b'course',\n",
       " b'enjoy',\n",
       " b'star',\n",
       " b'wanted',\n",
       " b'wife',\n",
       " b'directed',\n",
       " b'That',\n",
       " b'To',\n",
       " b'small',\n",
       " b'mind',\n",
       " b'seem',\n",
       " b'truly',\n",
       " b'While',\n",
       " b'stupid',\n",
       " b'simply',\n",
       " b\"'\",\n",
       " b'poor',\n",
       " b'All',\n",
       " b'worth',\n",
       " b'everyone',\n",
       " b'once',\n",
       " b'worse',\n",
       " b'top',\n",
       " b'completely',\n",
       " b'used',\n",
       " b'cinema',\n",
       " b'felt',\n",
       " b'kids',\n",
       " b'genre',\n",
       " b'shot',\n",
       " b'home',\n",
       " b'name',\n",
       " b'late',\n",
       " b'tell',\n",
       " b'called',\n",
       " b'until',\n",
       " b'reviews',\n",
       " b'New',\n",
       " b'myself',\n",
       " b'beginning',\n",
       " b'full',\n",
       " b'nice',\n",
       " b'live',\n",
       " b'left',\n",
       " b'At',\n",
       " b'horrible',\n",
       " b'novel',\n",
       " b'How',\n",
       " b'Although',\n",
       " b'thriller',\n",
       " b'documentary',\n",
       " b'women',\n",
       " b'starts',\n",
       " b'performances',\n",
       " b'several',\n",
       " b'else',\n",
       " b'Even',\n",
       " b'audience',\n",
       " b'less',\n",
       " b'perfect',\n",
       " b'often',\n",
       " b'couple',\n",
       " b'writing',\n",
       " b'amazing',\n",
       " b\"I'd\",\n",
       " b'review',\n",
       " b'released',\n",
       " b'seemed',\n",
       " b'history',\n",
       " b'need',\n",
       " b'case',\n",
       " b'Some',\n",
       " b'started',\n",
       " b'later',\n",
       " b'war',\n",
       " b'waste',\n",
       " b'Michael',\n",
       " b'try',\n",
       " b'use',\n",
       " b'either',\n",
       " b'given',\n",
       " b'humor',\n",
       " b'comments',\n",
       " b'B',\n",
       " b'black',\n",
       " b'disappointed',\n",
       " b'although',\n",
       " b'laugh',\n",
       " b'father',\n",
       " b'along',\n",
       " b'gave',\n",
       " b'however',\n",
       " b'quality',\n",
       " b'b',\n",
       " b'expect',\n",
       " b'totally',\n",
       " b'friend',\n",
       " b'gives',\n",
       " b'OK',\n",
       " b'itself',\n",
       " b'sex',\n",
       " b'thinking',\n",
       " b'surprised',\n",
       " b\"you're\",\n",
       " b'brilliant',\n",
       " b'With',\n",
       " b'Now',\n",
       " b'others',\n",
       " b'definitely',\n",
       " b'entire',\n",
       " b'dialogue',\n",
       " b'days',\n",
       " b'ending',\n",
       " b\"he's\",\n",
       " b'television',\n",
       " b'already',\n",
       " b'getting',\n",
       " b'past',\n",
       " b'let',\n",
       " b'James',\n",
       " b'boy',\n",
       " b'camera',\n",
       " b'style',\n",
       " b'problem',\n",
       " b'certainly',\n",
       " b'Just',\n",
       " b'group',\n",
       " b'stories',\n",
       " b'lead',\n",
       " b'o',\n",
       " b'town',\n",
       " b'Why',\n",
       " b'Of',\n",
       " b'maybe',\n",
       " b'person',\n",
       " b'himself',\n",
       " b'playing',\n",
       " b'lost',\n",
       " b'art',\n",
       " b'supposed',\n",
       " b'become',\n",
       " b'doing',\n",
       " b'usually',\n",
       " b'lives',\n",
       " b'men',\n",
       " b'looked',\n",
       " b'moments',\n",
       " b'death',\n",
       " b'hit',\n",
       " b'kid',\n",
       " b'took',\n",
       " b'slow',\n",
       " b'house',\n",
       " b'known',\n",
       " b'sort',\n",
       " b'fine',\n",
       " b'example',\n",
       " b'recently',\n",
       " b'picture',\n",
       " b'premise',\n",
       " b'An',\n",
       " b'His',\n",
       " b'crap',\n",
       " b'knew',\n",
       " b'British',\n",
       " b'direction',\n",
       " b'Robert',\n",
       " b'child',\n",
       " b'decided',\n",
       " b'hour',\n",
       " b'parts',\n",
       " b'Mr',\n",
       " b'help',\n",
       " b'huge',\n",
       " b'expecting',\n",
       " b'From',\n",
       " b'guess',\n",
       " b'fans',\n",
       " b'lines',\n",
       " b'Man',\n",
       " b'works',\n",
       " b'son',\n",
       " b'human',\n",
       " b'attempt',\n",
       " b'David',\n",
       " b'children',\n",
       " b'decent',\n",
       " b'comment',\n",
       " b'extremely',\n",
       " b'finally',\n",
       " b'sequel',\n",
       " b'jokes',\n",
       " b'keep',\n",
       " b'admit',\n",
       " b'actress',\n",
       " b'etc',\n",
       " b'writer',\n",
       " b\"there's\",\n",
       " b'episodes',\n",
       " b'against',\n",
       " b'type',\n",
       " b'game',\n",
       " b'simple',\n",
       " b'musical',\n",
       " b'heart',\n",
       " b'George',\n",
       " b'turned',\n",
       " b'Yes',\n",
       " b'tale',\n",
       " b'storyline',\n",
       " b'under',\n",
       " b'greatest',\n",
       " b'age',\n",
       " b'romantic',\n",
       " b'run',\n",
       " b'opinion',\n",
       " b'rest',\n",
       " b'hilarious',\n",
       " b'Richard',\n",
       " b'able',\n",
       " b'perhaps',\n",
       " b'expected',\n",
       " b'w',\n",
       " b'saying',\n",
       " b'close',\n",
       " b'theater',\n",
       " b'act',\n",
       " b'opening',\n",
       " b'involved',\n",
       " b'killer',\n",
       " b'told',\n",
       " b'typical',\n",
       " b\"I'll\",\n",
       " b'guys',\n",
       " b'Film',\n",
       " b'throughout',\n",
       " b'scary',\n",
       " b'face',\n",
       " b're',\n",
       " b'head',\n",
       " b'turn',\n",
       " b'usual',\n",
       " b'cannot',\n",
       " b'mother',\n",
       " b'serious',\n",
       " b'hours',\n",
       " b'York',\n",
       " b'Its',\n",
       " b'ridiculous',\n",
       " b'bunch',\n",
       " b'feeling',\n",
       " b'next',\n",
       " b'dark',\n",
       " b\"Don't\",\n",
       " b'wants',\n",
       " b'care',\n",
       " b'Unfortunately',\n",
       " b'On',\n",
       " b'predictable',\n",
       " b'word',\n",
       " b'side',\n",
       " b'chance',\n",
       " b'tries',\n",
       " b'car',\n",
       " b'Then',\n",
       " b'fi',\n",
       " b'today',\n",
       " b'Disney',\n",
       " b\"There's\",\n",
       " b'remake',\n",
       " b'self',\n",
       " b'rented',\n",
       " b'sometimes',\n",
       " b'Peter',\n",
       " b'br',\n",
       " b'class',\n",
       " b'dead',\n",
       " b'strange',\n",
       " b'local',\n",
       " b'across',\n",
       " b'acted',\n",
       " b'girls',\n",
       " b'cool',\n",
       " b'animation',\n",
       " b'middle',\n",
       " b'happened',\n",
       " b'silly',\n",
       " b'rating',\n",
       " b'stop',\n",
       " b'instead',\n",
       " b'modern',\n",
       " b\"haven't\",\n",
       " b'interest',\n",
       " b'sound',\n",
       " b'murder',\n",
       " b'doubt',\n",
       " b'matter',\n",
       " b'complete',\n",
       " b'exactly',\n",
       " b'fantastic',\n",
       " b'experience',\n",
       " b\"won't\",\n",
       " b'release',\n",
       " b'talent',\n",
       " b'stuff',\n",
       " b'living',\n",
       " b'white',\n",
       " b'IMDb',\n",
       " b'single',\n",
       " b'obviously',\n",
       " b'Tom',\n",
       " b'upon',\n",
       " b'enjoyable',\n",
       " b'four',\n",
       " b'five',\n",
       " b'except',\n",
       " b'S',\n",
       " b'tried',\n",
       " b'bought',\n",
       " b'reading',\n",
       " b'highly',\n",
       " b'm',\n",
       " b'coming',\n",
       " b'due',\n",
       " b'behind',\n",
       " b'call',\n",
       " b'Most',\n",
       " b'e',\n",
       " b'brother',\n",
       " b'eyes',\n",
       " b'French',\n",
       " b'wonder',\n",
       " b'War',\n",
       " b'number',\n",
       " b'shown',\n",
       " b'sad',\n",
       " b'tells',\n",
       " b'famous',\n",
       " b'feature',\n",
       " b'filmed',\n",
       " b'strong',\n",
       " b'becomes',\n",
       " b'somewhat',\n",
       " b'box',\n",
       " b'Oh',\n",
       " b'hope',\n",
       " b'wrote',\n",
       " b'season',\n",
       " b'obvious',\n",
       " b'happen',\n",
       " b'adaptation',\n",
       " b'clich',\n",
       " b'Paul',\n",
       " b'lame',\n",
       " b'Like',\n",
       " b'viewing',\n",
       " b'poorly',\n",
       " b'moving',\n",
       " b'career',\n",
       " b'evil',\n",
       " b'recommend',\n",
       " b'King',\n",
       " b'husband',\n",
       " b'Oscar',\n",
       " b'named',\n",
       " b'viewer',\n",
       " b'begin',\n",
       " b'violence',\n",
       " b'write',\n",
       " b'non',\n",
       " b'ones',\n",
       " b'view',\n",
       " b'expectations',\n",
       " b'among',\n",
       " b'dull',\n",
       " b'wish',\n",
       " b'masterpiece',\n",
       " b'major',\n",
       " b'straight',\n",
       " b'agree',\n",
       " b'Movie',\n",
       " b'sit',\n",
       " b'hand',\n",
       " b'period',\n",
       " b'directing',\n",
       " b'particularly',\n",
       " b'country',\n",
       " b'fight',\n",
       " b'hate',\n",
       " b'daughter',\n",
       " b'happens',\n",
       " b'English',\n",
       " b'says',\n",
       " b'stand',\n",
       " b'turns',\n",
       " b'Christmas',\n",
       " b'f',\n",
       " b'attention',\n",
       " b\"That's\",\n",
       " b'taken',\n",
       " b'cheesy',\n",
       " b'comic',\n",
       " b'mostly',\n",
       " b\"wouldn't\",\n",
       " b'light',\n",
       " b'entertainment',\n",
       " b'order',\n",
       " b'roles',\n",
       " b'superb',\n",
       " b'dialog',\n",
       " b'easily',\n",
       " b'starring',\n",
       " b'Lee',\n",
       " b'weak',\n",
       " b'mystery',\n",
       " b'Night',\n",
       " b'seriously',\n",
       " b'realistic',\n",
       " b'reasons',\n",
       " b'theme',\n",
       " b'possibly',\n",
       " b'words',\n",
       " b'ten',\n",
       " b'cheap',\n",
       " b'similar',\n",
       " b'brought',\n",
       " b'co',\n",
       " b'including',\n",
       " b'nearly',\n",
       " b'cinematography',\n",
       " b'falls',\n",
       " b'basically',\n",
       " b'produced',\n",
       " b'talking',\n",
       " b'working',\n",
       " b'taking',\n",
       " b'reality',\n",
       " b'store',\n",
       " b'unique',\n",
       " b'forward',\n",
       " b'hell',\n",
       " b'alone',\n",
       " b'T',\n",
       " b'whose',\n",
       " b'sets',\n",
       " b'previous',\n",
       " b'soon',\n",
       " b'themselves',\n",
       " b'lack',\n",
       " b'killed',\n",
       " b'directors',\n",
       " b'c',\n",
       " b'bring',\n",
       " b'week',\n",
       " b'effort',\n",
       " b'average',\n",
       " b'ways',\n",
       " b'female',\n",
       " b'running',\n",
       " b'fast',\n",
       " b'editing',\n",
       " b'kept',\n",
       " b'caught',\n",
       " b'Japanese',\n",
       " b'popular',\n",
       " b'screenplay',\n",
       " b'elements',\n",
       " b'comedies',\n",
       " b'surprise',\n",
       " b'became',\n",
       " b'problems',\n",
       " b'Jack',\n",
       " b'features',\n",
       " b'annoying',\n",
       " b'animated',\n",
       " b'yes',\n",
       " b'd',\n",
       " b'possible',\n",
       " b'cover',\n",
       " b'follow',\n",
       " b'near',\n",
       " b'William',\n",
       " b'list',\n",
       " b'songs',\n",
       " b'cartoon',\n",
       " b'Having',\n",
       " b'voice',\n",
       " b'parents',\n",
       " b'Maybe',\n",
       " b'Star',\n",
       " b'events',\n",
       " b'none',\n",
       " b'body',\n",
       " b'crime',\n",
       " b'h',\n",
       " b'Great',\n",
       " b'Who',\n",
       " b'future',\n",
       " b'Very',\n",
       " b'weird',\n",
       " b'God',\n",
       " b\"aren't\",\n",
       " b'actual',\n",
       " b'despite',\n",
       " b'final',\n",
       " b'Is',\n",
       " b'gore',\n",
       " b'imagine',\n",
       " b'change',\n",
       " b'sister',\n",
       " b'cut',\n",
       " b\"you'll\",\n",
       " b'D',\n",
       " b'lots',\n",
       " b'beyond',\n",
       " b'moment',\n",
       " b'Dr',\n",
       " b'positive',\n",
       " b'level',\n",
       " b'fantasy',\n",
       " b'important',\n",
       " b'minute',\n",
       " b'sci',\n",
       " b'Joe',\n",
       " b'easy',\n",
       " b'era',\n",
       " b'House',\n",
       " b'awesome',\n",
       " b'romance',\n",
       " b'above',\n",
       " b'team',\n",
       " b'subject',\n",
       " b'rent',\n",
       " b'badly',\n",
       " b'happy',\n",
       " b'finds',\n",
       " b'points',\n",
       " b'rate',\n",
       " b\"'The\",\n",
       " b'begins',\n",
       " b'message',\n",
       " b'incredibly',\n",
       " b'concept',\n",
       " b'Italian',\n",
       " b'realize',\n",
       " b'Good',\n",
       " b'recent',\n",
       " b'score',\n",
       " b'By',\n",
       " b'casting',\n",
       " b'America',\n",
       " b'police',\n",
       " b'SPOILERS',\n",
       " b'power',\n",
       " b\"you've\",\n",
       " b'NOT',\n",
       " b\"they're\",\n",
       " b'giving',\n",
       " b'college',\n",
       " b'difficult',\n",
       " b'fall',\n",
       " b'laughed',\n",
       " b'knows',\n",
       " b'save',\n",
       " b'incredible',\n",
       " b'third',\n",
       " b'potential',\n",
       " b'leave',\n",
       " b'song',\n",
       " b'clearly',\n",
       " b'Scott',\n",
       " b'slasher',\n",
       " b'talk',\n",
       " b'laughing',\n",
       " b'Every',\n",
       " b'meets',\n",
       " b'move',\n",
       " b'fairly',\n",
       " b'kill',\n",
       " b'hero',\n",
       " b'plain',\n",
       " b'means',\n",
       " b'p',\n",
       " b'adventure',\n",
       " b'suspense',\n",
       " b'certain',\n",
       " b'joke',\n",
       " b'material',\n",
       " b'rated',\n",
       " b'showing',\n",
       " b'Also',\n",
       " b'mess',\n",
       " b'needs',\n",
       " b'World',\n",
       " b'Ben',\n",
       " b'gone',\n",
       " b'books',\n",
       " b'disappointment',\n",
       " b'cop',\n",
       " b'talented',\n",
       " b'copy',\n",
       " b'Bill',\n",
       " b'Festival',\n",
       " b'showed',\n",
       " b'success',\n",
       " b'interested',\n",
       " b'fails',\n",
       " b'shots',\n",
       " b'stage',\n",
       " b'describe',\n",
       " b'Wow',\n",
       " b'Okay',\n",
       " b\"she's\",\n",
       " b'wasted',\n",
       " b'Jane',\n",
       " b'particular',\n",
       " b'spent',\n",
       " b'whether',\n",
       " b'bored',\n",
       " b'wait',\n",
       " b'clear',\n",
       " b'rare',\n",
       " b'Jim',\n",
       " b'trash',\n",
       " b'consider',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truncated_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b5d6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f11e2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c123497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0e6a8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = datasets[\"train\"].batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5cb596cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "14f4ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e38f948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 61s 74ms/step - loss: 0.6603 - accuracy: 0.5754\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 58s 74ms/step - loss: 0.4916 - accuracy: 0.7484\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 60s 76ms/step - loss: 0.3460 - accuracy: 0.8517\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 59s 75ms/step - loss: 0.2611 - accuracy: 0.8980\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 61s 78ms/step - loss: 0.1863 - accuracy: 0.9326\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05969489",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5821ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=keras.layers.Input(shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2b6595e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n",
    "z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n",
    "z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n",
    "z = keras.layers.GRU(128)(z, mask=mask)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51ccb553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=[inputs], outputs=[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edbe0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e4ab46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\", dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10465d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7537746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe.descriptor.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\saved_model.pb\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\assets\\tokens.txt\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.data-00000-of-00001\n",
      ".\\my_tfhub_cache\\82c4aaf4250ffb09088bd48368ee7fd00e5464fe\\variables\\variables.index\n"
     ]
    }
   ],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "850618b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c40dc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbaf8c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.5456 - accuracy: 0.7264\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5145 - accuracy: 0.7471\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5091 - accuracy: 0.7504\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5054 - accuracy: 0.7528\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.5026 - accuracy: 0.7540\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets[\"train\"].batch(batch_size).prefetch(1)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "deaf7ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\mlbook\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "41e5b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "ec_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "\n",
    "dc_inputs = keras.layers.Input(shape=[None], dtype=np.int32) \n",
    "\n",
    "sequence_lenght = keras.layers.Input(shape=[], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d41674c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "\n",
    "ec_embeddigs = embeddings(ec_inputs)\n",
    "dc_embeddings = embeddings(dc_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f570e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecoder = keras.layers.LSTM(512, return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f0f3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_reber_grammar = [\n",
    "    [(\"B\", 1)],           # (state 0) =B=>(state 1)\n",
    "    [(\"T\", 2), (\"P\", 3)], # (state 1) =T=>(state 2) or =P=>(state 3)\n",
    "    [(\"S\", 2), (\"X\", 4)], # (state 2) =S=>(state 2) or =X=>(state 4)\n",
    "    [(\"T\", 3), (\"V\", 5)], # and so on...\n",
    "    [(\"X\", 3), (\"S\", 6)],\n",
    "    [(\"P\", 4), (\"V\", 6)],\n",
    "    [(\"E\", None)]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65a01f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_reber_grammar = [\n",
    "    [(\"B\", 1)],\n",
    "    [(\"T\", 2), (\"P\", 3)],\n",
    "    [(default_reber_grammar, 4)],\n",
    "    [(default_reber_grammar, 5)],\n",
    "    [(\"T\", 6)],\n",
    "    [(\"P\", 6)],\n",
    "    [(\"E\", None)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad24031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_string(grammar):\n",
    "    state=0\n",
    "    output=[]\n",
    "    while state is not None:\n",
    "        index = np.random.randint(len(grammar[state]))\n",
    "        production, state = grammar[state][index]\n",
    "        if isinstance(production, list):\n",
    "            production = generate_string(grammar=production)\n",
    "        output.append(production)\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbb54cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPTTTVPXVVE BTXSE BTSSSSSXXTTTVPXVVE BPTVPSE BTXXTTVVE BPVPSE BPTVVE BTXXVPXTVPXVPSE BPVVE BTXSE BTXXTVPXTVVE BTSXSE BPVVE BTSSXSE BTXXVVE BTSXXVVE BTSSSXXTTTTVVE BPTTTVVE BPTVVE BTXSE BPTVPXVPSE BTSXSE BTXSE BPTTTTTTVPXVPXTTVVE BPTTTVPXVPXTTVVE "
     ]
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    print(generate_string(default_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3fd8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSSIBLE_CHARS = \"BEPSTVX\"\n",
    "def generate_corrupted_string(grammar, chars=POSSIBLE_CHARS):\n",
    "    good_string = generate_string(grammar)\n",
    "    index = np.random.randint(len(good_string))\n",
    "    good_char = good_string[index]\n",
    "    bad_char = np.random.choice(sorted(set(chars) - set(good_char)))\n",
    "    return good_string[:index] + bad_char + good_string[index + 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "871000dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XPBPVVEPE BTBTXXTTTTTTXPSETE BPBTXXTTTBVVEPE BPBPVPXVPXVESEPE BPVTSXSEPE BPETXSEPE BPBTXSESE ETBPVVETE BTBTXXTVPEVVETE BTBTSXXVPXTVTSETE STBTXSETE BTSTXSETE BPVTSXXVVEPE BPBTSSXSEEE BPBTXVEPE BPBTSSXSPPE BPBTSSXXTTVPSETE BPBTVXSEPE STBPTVPXVPSETE BPXTSSXSEPE BTBVXXVPXTTTTTTTTVPSETE BTBBVPSETE BTBPTTTTVVTTE BTBTXXTTTTTVPXVVEXE BPBTSSSSSSEPE "
     ]
    }
   ],
   "source": [
    "for _ in range(25):\n",
    "    print(generate_corrupted_string(embedded_reber_grammar), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46f0d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_ids(s, chars=POSSIBLE_CHARS):\n",
    "    return [chars.index(c) for c in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "835e6c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 4, 4, 6, 6, 5, 5, 1, 4, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_to_ids(\"BTTTXXVVETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96a01a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def generate_dataset(size):\n",
    "    good_strings = [string_to_ids(generate_string(embedded_reber_grammar))\n",
    "                    for _ in range(size // 2)]\n",
    "    bad_strings = [string_to_ids(generate_corrupted_string(embedded_reber_grammar))\n",
    "                   for _ in range(size - size // 2)]\n",
    "    all_strings = good_strings + bad_strings\n",
    "    X = tf.ragged.constant(all_strings, ragged_rank=1)\n",
    "    y = np.array([[1.] for _ in range(len(good_strings))] +\n",
    "                 [[0.] for _ in range(len(bad_strings))])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7676971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = generate_dataset(10000)\n",
    "X_valid, y_valid = generate_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2859f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "338eee15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15,), dtype=int32, numpy=array([0, 4, 0, 2, 4, 5, 2, 6, 4, 5, 2, 3, 1, 4, 1])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f639913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a580a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 5\n",
    "from tensorflow import keras\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[None], dtype=tf.int32, ragged=True),\n",
    "    keras.layers.Embedding(input_dim=len(POSSIBLE_CHARS), output_dim=embedding_size),\n",
    "    keras.layers.GRU(30),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3623c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e15f06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6f8cba53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.3365 - accuracy: 0.8646 - val_loss: 0.4592 - val_accuracy: 0.7175\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2053 - accuracy: 0.9321 - val_loss: 0.1129 - val_accuracy: 0.9735\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1182 - accuracy: 0.9650 - val_loss: 0.0844 - val_accuracy: 0.9805\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0918 - accuracy: 0.9774 - val_loss: 0.1764 - val_accuracy: 0.9480\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.1926 - accuracy: 0.9303 - val_loss: 0.1040 - val_accuracy: 0.9745\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0885 - accuracy: 0.9786 - val_loss: 0.0849 - val_accuracy: 0.9790\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0800 - accuracy: 0.9805 - val_loss: 0.0704 - val_accuracy: 0.9815\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0455 - accuracy: 0.9873 - val_loss: 0.0148 - val_accuracy: 0.9980\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 8.1170e-04 - accuracy: 1.0000 - val_loss: 7.2789e-04 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 4.9744e-04 - accuracy: 1.0000 - val_loss: 4.6742e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.5722e-04 - accuracy: 1.0000 - val_loss: 3.4646e-04 - val_accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.7941e-04 - accuracy: 1.0000 - val_loss: 2.9199e-04 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 2.3171e-04 - accuracy: 1.0000 - val_loss: 2.3501e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.9627e-04 - accuracy: 1.0000 - val_loss: 2.0534e-04 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.7192e-04 - accuracy: 1.0000 - val_loss: 1.7827e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.5274e-04 - accuracy: 1.0000 - val_loss: 1.5872e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.3747e-04 - accuracy: 1.0000 - val_loss: 1.4429e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 1.2474e-04 - accuracy: 1.0000 - val_loss: 1.3151e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 1.1442e-04 - accuracy: 1.0000 - val_loss: 1.2078e-04 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13d762b7fa0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "09ae3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_strings = [\"BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE\",\n",
    "                \"BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1b3b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tf.ragged.constant([string_to_ids(s) for s in test_strings], ragged_rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7b76dc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "y_proba = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0bfd0e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.001542 ],\n",
       "       [0.9998921]], dtype=float32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d5c2e9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVETE: 0.15%\n",
      "BPBTSSSSSSSXXTTVPXVPXTTTTTVVEPE: 99.99%\n"
     ]
    }
   ],
   "source": [
    "for index, string in enumerate(test_strings):\n",
    "    print(\"{}: {:.2f}%\".format(string, 100 * y_proba[index][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6644c92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "          \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "28b000aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "June 15, 1903\n",
      "May 27, 2008\n",
      "December 18, 1970\n",
      "May 27, 1991\n",
      "July 03, 1995\n"
     ]
    }
   ],
   "source": [
    "import calendar\n",
    "from datetime import date\n",
    "import random\n",
    "\n",
    "def generate_random_date():\n",
    "    # Gera um ano aleatório\n",
    "    year = random.randint(1900, 2024)\n",
    "    \n",
    "    # Gera um mês aleatório\n",
    "    month = random.randint(1, 12)\n",
    "    \n",
    "    # Obtém o número de dias do mês e ano gerado\n",
    "    _, num_days = calendar.monthrange(year, month)\n",
    "    \n",
    "    # Gera um dia aleatório dentro do intervalo válido para o mês\n",
    "    day = random.randint(1, num_days)\n",
    "    \n",
    "    # Cria a data\n",
    "    random_date = date(year, month, day)\n",
    "    \n",
    "    # Converte a data para o formato desejado\n",
    "    date_str = random_date.strftime(\"%B %d, %Y\")\n",
    "    return date_str\n",
    "\n",
    "# Teste\n",
    "for _ in range(5):\n",
    "    print(generate_random_date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "4689c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = generate_random_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "d1747c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "December 27, 1998\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c0ed2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def convert_format(date_str):\n",
    "    date_obj = datetime.strptime(date_str, '%B %d, %Y')\n",
    "    return date_obj.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "2f3040a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "March 06, 1993\n",
      "May 29, 1938\n",
      "October 03, 1994\n",
      "November 16, 1954\n",
      "September 16, 1970\n",
      "November 27, 1905\n",
      "August 29, 2021\n",
      "January 31, 1956\n",
      "December 31, 1937\n",
      "April 19, 1902\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "for _ in range(10):\n",
    "    dates = generate_random_date()\n",
    "    print(dates)\n",
    "    new_date = convert_format(dates)\n",
    "    examples.append([dates, new_date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "c6439065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['March 06, 1993', '1993-03-06'],\n",
       " ['May 29, 1938', '1938-05-29'],\n",
       " ['October 03, 1994', '1994-10-03'],\n",
       " ['November 16, 1954', '1954-11-16'],\n",
       " ['September 16, 1970', '1970-09-16'],\n",
       " ['November 27, 1905', '1905-11-27'],\n",
       " ['August 29, 2021', '2021-08-29'],\n",
       " ['January 31, 1956', '1956-01-31'],\n",
       " ['December 31, 1937', '1937-12-31'],\n",
       " ['April 19, 1902', '1902-04-19']]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a7ce41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(n_samples):\n",
    "    X = []\n",
    "    y = []\n",
    "    for _ in range(n_samples):\n",
    "        date_str = generate_random_date()\n",
    "        X.append(date_str)\n",
    "        target = convert_format(date_str)\n",
    "        y.append(target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a264042f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9f288c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['August 09, 1968',\n",
       " 'December 30, 1988',\n",
       " 'November 20, 1933',\n",
       " 'November 15, 1954',\n",
       " 'May 18, 1974',\n",
       " 'December 16, 1912',\n",
       " 'December 12, 1916',\n",
       " 'January 28, 1998',\n",
       " 'September 14, 2019',\n",
       " 'June 08, 1984',\n",
       " 'December 17, 1922',\n",
       " 'August 22, 1990',\n",
       " 'May 24, 1952',\n",
       " 'April 22, 1953',\n",
       " 'December 07, 1965',\n",
       " 'September 18, 1916',\n",
       " 'October 21, 2003',\n",
       " 'November 27, 1952',\n",
       " 'October 28, 1997',\n",
       " 'June 16, 1937',\n",
       " 'September 09, 1946',\n",
       " 'September 19, 1951',\n",
       " 'July 14, 1956',\n",
       " 'September 12, 2021',\n",
       " 'January 07, 1972',\n",
       " 'June 06, 1981',\n",
       " 'July 03, 1996',\n",
       " 'April 19, 1956',\n",
       " 'July 17, 1911',\n",
       " 'February 15, 1987',\n",
       " 'January 14, 1972',\n",
       " 'October 28, 1933',\n",
       " 'October 07, 2020',\n",
       " 'November 17, 1979',\n",
       " 'October 02, 1931',\n",
       " 'May 02, 1942',\n",
       " 'October 31, 1962',\n",
       " 'May 27, 1996',\n",
       " 'March 29, 1934',\n",
       " 'March 19, 1949',\n",
       " 'July 14, 1902',\n",
       " 'August 14, 1980',\n",
       " 'May 27, 1911',\n",
       " 'December 23, 1922',\n",
       " 'February 11, 1944',\n",
       " 'May 29, 1950',\n",
       " 'January 30, 1917',\n",
       " 'May 21, 2006',\n",
       " 'February 11, 2005',\n",
       " 'September 01, 1903',\n",
       " 'October 07, 2016',\n",
       " 'March 10, 1923',\n",
       " 'April 22, 1909',\n",
       " 'March 02, 1982',\n",
       " 'September 25, 1988',\n",
       " 'December 02, 2005',\n",
       " 'April 29, 2000',\n",
       " 'July 30, 1943',\n",
       " 'January 07, 1914',\n",
       " 'November 20, 1949',\n",
       " 'August 03, 2015',\n",
       " 'December 26, 2000',\n",
       " 'May 14, 1939',\n",
       " 'December 12, 2009',\n",
       " 'April 01, 1953',\n",
       " 'February 21, 2014',\n",
       " 'October 02, 1901',\n",
       " 'March 27, 1939',\n",
       " 'December 11, 1974',\n",
       " 'September 30, 1951',\n",
       " 'January 07, 2007',\n",
       " 'June 10, 1929',\n",
       " 'February 19, 1923',\n",
       " 'September 19, 1942',\n",
       " 'May 27, 2003',\n",
       " 'February 02, 1962',\n",
       " 'August 01, 1955',\n",
       " 'August 25, 1944',\n",
       " 'January 13, 1996',\n",
       " 'February 04, 1977',\n",
       " 'June 11, 1932',\n",
       " 'June 23, 1993',\n",
       " 'January 18, 1970',\n",
       " 'September 24, 1990',\n",
       " 'September 22, 1990',\n",
       " 'April 21, 1901',\n",
       " 'July 02, 1982',\n",
       " 'December 03, 1969',\n",
       " 'June 10, 1932',\n",
       " 'February 15, 1946',\n",
       " 'June 04, 1961',\n",
       " 'August 05, 1952',\n",
       " 'August 12, 1970',\n",
       " 'January 05, 1913',\n",
       " 'October 19, 1938',\n",
       " 'March 06, 1966',\n",
       " 'September 23, 1908',\n",
       " 'January 26, 1949',\n",
       " 'July 22, 1998',\n",
       " 'October 04, 1915']"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 100\n",
    "test1, test1_y = create_dataset(n_samples)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "7b8569c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"2\" in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "622f1573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['J', 'a', 'n', 'u', 'r', 'y', 'F', 'e', 'b', 'M', 'c', 'h', 'A', 'p', 'i', 'l', 'g', 's', 't', 'S', 'm', 'O', 'o', 'N', 'v', 'D']\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for month in MONTHS:\n",
    "    for char in month:\n",
    "        if char not in vocab:\n",
    "            vocab.append(char)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "670e252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    vocab.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "83aa93b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J',\n",
       " 'a',\n",
       " 'n',\n",
       " 'u',\n",
       " 'r',\n",
       " 'y',\n",
       " 'F',\n",
       " 'e',\n",
       " 'b',\n",
       " 'M',\n",
       " 'c',\n",
       " 'h',\n",
       " 'A',\n",
       " 'p',\n",
       " 'i',\n",
       " 'l',\n",
       " 'g',\n",
       " 's',\n",
       " 't',\n",
       " 'S',\n",
       " 'm',\n",
       " 'O',\n",
       " 'o',\n",
       " 'N',\n",
       " 'v',\n",
       " 'D',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0f29effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 52, 54, 45, 48, 63, 2, 2, 64, 63, 2, 0, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "INPUT_CHARS = \"0123456789-ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz ,\"\n",
    "\n",
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]\n",
    "\n",
    "# Exemplo com uma string de data\n",
    "date_str = \"April 22, 2019\"\n",
    "date_ids = date_str_to_ids(date_str, INPUT_CHARS)\n",
    "print(date_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b1fe1b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 52, 54, 45, 48, 63, 2, 1, 64, 63, 2, 0, 1, 9]\n"
     ]
    }
   ],
   "source": [
    "date_str = \"April 21, 2019\"\n",
    "date_ids = date_str_to_ids(date_str, INPUT_CHARS)\n",
    "print(date_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d93b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
