{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bd28a7af-510c-4dc4-97e5-bf78188d0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5d644d41-fa47-4e04-be4a-e831568e7fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9910e1a4-d248-498a-851b-b4f9a1d5295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "249d99eb-7d3f-4973-8d7b-69cc3b091534",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2a160a9f-881e-4254-9811-e3fa545e8813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "215f2e33-5b45-4a41-9401-07a7e8c46cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5ee5c3cf-d95c-4880-a6a5-665daf0f3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a383dbd1-5df3-490c-b94a-e9212cc75140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "048ac134-126b-4eb8-a5bc-2d5399e80380",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2ad02bdf-e9e7-4ae2-84b3-18a40013f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6237a980-a400-4d9c-b481-8bf853a8d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bef210e8-86a1-4c7a-955a-45e7017cd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5960 - val_loss: 0.2346\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 918us/step - loss: 0.2154 - val_loss: 0.1993\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 904us/step - loss: 0.2032 - val_loss: 0.2029\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 889us/step - loss: 0.1985 - val_loss: 0.1896\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 972us/step - loss: 0.1954 - val_loss: 0.1857\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 916us/step - loss: 0.1915 - val_loss: 0.2032\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.1909 - val_loss: 0.1882\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 885us/step - loss: 0.1883 - val_loss: 0.1813\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 898us/step - loss: 0.1863 - val_loss: 0.1763\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 883us/step - loss: 0.1841 - val_loss: 0.1846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8a2a228f0>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07754415-5e4e-41b7-b2cc-f606c2d6a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "class LayerNormalizationn(keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            shape=(input_shape[-1:]), name=\"alpha\", initializer=\"ones\"\n",
    "        )\n",
    "        self.beta = self.add_weight(shape=(input_shape), name=\"beta\", initializer=\"zeros\")\n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        std_deviation = tf.sqrt(variance + self.eps)\n",
    "        return self.alpha * (X - mean)/(std_deviation) + self.beta\n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b9cbc0d6-5da0-47b6-92f3-620384768c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.9357733e-08>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer_norm = LayerNormalizationn()\n",
    "\n",
    "keras_layer_norm = keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(keras.losses.mean_absolute_error(keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fb0f4a0c-4e45-4b75-881b-425d92a55e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "b39207b5-3785-4bfd-befe-f391941f45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.0\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "def random_batch(X, y, batch_size):\n",
    "    idx = np.random.randint(0, len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e99aca85-73f0-40a4-9258-572368b417b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88bd206b6884341ad0363358b248fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|                                                                                | 0/5 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598e224672624c3a9bf0f7ce0f15a631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - val_loss: 0.4300 - val_accuracy: 0.8496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb96ac1146c45959716a184d070cbcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - val_loss: 0.4251 - val_accuracy: 0.8506\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b4fb94b3c24777b8c68b4c3ca27ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - val_loss: 0.4150 - val_accuracy: 0.8580\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cac71e2f7944c5ae63e5d4b5d47b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - val_loss: 0.3926 - val_accuracy: 0.8630\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3121ea49b60c49f58757ddedc33b22dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - val_loss: 0.3882 - val_accuracy: 0.8730\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # Usando auto para adaptar ao ambiente (notebook, terminal, etc)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Barra de progresso para os epochs\n",
    "for epoch in tqdm(range(1, n_epochs + 1), desc=\"All epochs\", dynamic_ncols=True):\n",
    "    mean_loss.reset_states()\n",
    "\n",
    "    # Barra de progresso para os steps dentro de cada epoch\n",
    "    step_bar = tqdm(range(1, n_steps + 1), desc=f\"Epoch {epoch}/{n_epochs}\", leave=False, dynamic_ncols=True)\n",
    "    for step in step_bar:\n",
    "        X_batch, y_batch = random_batch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        status = OrderedDict()\n",
    "        status[\"loss\"] = mean_loss.result().numpy()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "            status[metric.name] = metric.result().numpy()\n",
    "\n",
    "        step_bar.set_postfix(status)\n",
    "\n",
    "    y_pred = model(X_valid)\n",
    "    val_loss = np.mean(loss_fn(y_valid, y_pred))\n",
    "    val_accuracy = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
    "        tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "\n",
    "    tqdm.write(f\"Epoch {epoch}/{n_epochs} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "62018d6c-b2e7-4936-853a-64f706aea6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "lower_layers = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\")\n",
    "])\n",
    "\n",
    "upper_layers = keras.models.Sequential([\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    lower_layers, upper_layers\n",
    "])\n",
    "\n",
    "lower_optimizer = keras.optimizers.SGD(learning_rate=0.0001)\n",
    "upper_optimizer = keras.optimizers.Nadam(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c604bc8a-118b-4342-a477-46da4f2501ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4b9e1851a748978b78f64cb395e355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "All epochs:   0%|                                                                                | 0/5 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c67020d9e054c67ba208a9f5d377c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - val_loss: 0.5786 - val_accuracy: 0.7888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614f8af0786c4363a99b34b44533dc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 - val_loss: 0.5190 - val_accuracy: 0.8112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8cd8612c6b4323a3aea2e177925eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 - val_loss: 0.5595 - val_accuracy: 0.8142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a29f6ac6644412c92f0510d72dee1f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 - val_loss: 0.5170 - val_accuracy: 0.8306\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2708bb1d49de4239995e730afb2110bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/5:   0%|                                                                              | 0/1718 [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 - val_loss: 0.4843 - val_accuracy: 0.8368\n"
     ]
    }
   ],
   "source": [
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "n_epochs = 5\n",
    "for epoch in tqdm(range(1, n_epochs + 1), desc=\"All epochs\", dynamic_ncols=True):\n",
    "    mean_loss.reset_states()\n",
    "\n",
    "    step_bar = tqdm(range(1, n_steps + 1), desc=f\"Epoch {epoch}/{n_epochs}\", leave=False, dynamic_ncols=True)\n",
    "    for step in step_bar:\n",
    "        X_batch, y_batch = random_batch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        for layers, optimizers in ((lower_layers, lower_optimizer), (upper_layers, upper_optimizer)):\n",
    "            gradients = tape.gradient(loss, layers.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, layers.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        status = OrderedDict()\n",
    "        status[\"loss\"] = mean_loss.result().numpy()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "            status[metric.name] = metric.result().numpy()\n",
    "\n",
    "        step_bar.set_postfix(status)\n",
    "\n",
    "    y_pred = model(X_valid)\n",
    "    val_loss = np.mean(loss_fn(y_valid, y_pred))\n",
    "    val_accuracy = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
    "        tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "\n",
    "    tqdm.write(f\"Epoch {epoch}/{n_epochs} - val_loss: {val_loss:.4f} - val_accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c250f-05c6-4717-9fb8-8f12c0e6c191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
