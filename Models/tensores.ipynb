{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd28a7af-510c-4dc4-97e5-bf78188d0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d644d41-fa47-4e04-be4a-e831568e7fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 2., 3.],\n",
       "       [4., 5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9910e1a4-d248-498a-851b-b4f9a1d5295f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249d99eb-7d3f-4973-8d7b-69cc3b091534",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a160a9f-881e-4254-9811-e3fa545e8813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[2., 3.],\n",
       "       [5., 6.]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "215f2e33-5b45-4a41-9401-07a7e8c46cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 5.], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[..., 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee5c3cf-d95c-4880-a6a5-665daf0f3088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a383dbd1-5df3-490c-b94a-e9212cc75140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "048ac134-126b-4eb8-a5bc-2d5399e80380",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ad02bdf-e9e7-4ae2-84b3-18a40013f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6237a980-a400-4d9c-b481-8bf853a8d1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef210e8-86a1-4c7a-955a-45e7017cd619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "363/363 [==============================] - 1s 993us/step - loss: 0.5744 - val_loss: 0.2720\n",
      "Epoch 2/10\n",
      "363/363 [==============================] - 0s 793us/step - loss: 0.2150 - val_loss: 0.2368\n",
      "Epoch 3/10\n",
      "363/363 [==============================] - 0s 758us/step - loss: 0.2045 - val_loss: 0.2050\n",
      "Epoch 4/10\n",
      "363/363 [==============================] - 0s 709us/step - loss: 0.1985 - val_loss: 0.1958\n",
      "Epoch 5/10\n",
      "363/363 [==============================] - 0s 712us/step - loss: 0.1939 - val_loss: 0.1935\n",
      "Epoch 6/10\n",
      "363/363 [==============================] - 0s 727us/step - loss: 0.1901 - val_loss: 0.1947\n",
      "Epoch 7/10\n",
      "363/363 [==============================] - 0s 719us/step - loss: 0.1869 - val_loss: 0.1753\n",
      "Epoch 8/10\n",
      "363/363 [==============================] - 0s 777us/step - loss: 0.1836 - val_loss: 0.1728\n",
      "Epoch 9/10\n",
      "363/363 [==============================] - 0s 728us/step - loss: 0.1807 - val_loss: 0.1767\n",
      "Epoch 10/10\n",
      "363/363 [==============================] - 0s 711us/step - loss: 0.1782 - val_loss: 0.1668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff8beffd00>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=10,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07754415-5e4e-41b7-b2cc-f606c2d6a723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11610, 8)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "class LayerNormalizationn(keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "    def build(self, input_shape):\n",
    "        self.alpha = self.add_weight(\n",
    "            shape=(input_shape[-1:]), name=\"alpha\", initializer=\"ones\"\n",
    "        )\n",
    "        self.beta = self.add_weight(shape=(input_shape), name=\"beta\", initializer=\"zeros\")\n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        std_deviation = tf.sqrt(variance + self.eps)\n",
    "        return self.alpha * (X - mean)/(std_deviation) + self.beta\n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9cbc0d6-5da0-47b6-92f3-620384768c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.9357733e-08>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_layer_norm = LayerNormalizationn()\n",
    "\n",
    "keras_layer_norm = keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(keras.losses.mean_absolute_error(keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fb0f4a0c-4e45-4b75-881b-425d92a55e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b39207b5-3785-4bfd-befe-f391941f45e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.0\n",
    "\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "def random_batch(X, y, batch_size):\n",
    "    idx = np.random.randint(0, len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e99aca85-73f0-40a4-9258-572368b417b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}')\n",
    "\n",
    "    mean_loss.reset_states()\n",
    "    for step in range(n_steps):\n",
    "        X_batch, y_batch = random_batch(X_train, y_train, batch_size)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "            gradients = tape.gradient(loss, model.treinable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.treinable_variables))\n",
    "            mean_loss(loss)\n",
    "            status = OrderedDict()\n",
    "            status[\"loss\"] = mean_loss.result().numpy()\n",
    "            for metric in metrics:\n",
    "                metric(y_batch, y_pred)\n",
    "                status[metric.name] = metric.result.numpy()\n",
    "            steps.set_postfix(status)\n",
    "        y_pred = model(X_valid)\n",
    "        status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "        status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
    "            tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "        steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325087c-afc2-4aa3-b456-27e1adbd8f6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
